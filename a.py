#!/usr/bin/env python3
#
# This code has been produced by a free evaluation version of Brainome(tm).
# Portions of this code copyright (c) 2019-2022 by Brainome, Inc. All Rights Reserved.
# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),
# worldwide, non-sublicensable, and non-transferable limited license to use and modify this
# predictor produced through the input of your data:
# (i) for users accessing the service through a free evaluation account, solely for your
# own non-commercial purposes, including for the purpose of evaluating this service, and
# (ii) for users accessing the service through a paid, commercial use account, for your
# own internal  and commercial purposes.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome v2.0-172-prod.
# Invocation: brainome -headerless -f NN -split 80 -rank 100 dataset.csv
# Total compiler execution time: 0:27:48.18. Finished on: May-11-2023 12:56:26.
# This source code requires Python 3.
#
"""

[01;1mPredictor:[0m                        
    Classifier Type:              Neural Network
    System Type:                  Binary classifier
    Training / Validation Split:  80% : 20%
    Accuracy:
      Best-guess accuracy:        50.00%
      Training accuracy:          64.06% (615/960 correct)
      Validation Accuracy:        60.00% (144/240 correct)
      Combined Model Accuracy:    63.25% (759/1200 correct)


    Model Capacity (MEC):       172    bits
    Generalization Ratio:         3.57 bits/bit
    Percent of Data Memorized:    56.02%
    Resilience to Noise:          -0.55 dB




    Training Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   0 |   322   158 
                   1 |   187   293 

    Validation Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   0 |    77    43 
                   1 |    53    67 

    Training Accuracy by Class:
                7056 |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
                ---- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                   0 |   322   187   293   158   67.08%   61.04%   63.26%   64.97%   65.12%   48.28%
                   1 |   293   158   322   187   61.04%   67.08%   64.97%   63.26%   62.94%   45.92%

    Validation Accuracy by Class:
                7056 |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
                ---- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                   0 |    77    53    67    43   64.17%   55.83%   59.23%   60.91%   61.60%   44.51%
                   1 |    67    43    77    53   55.83%   64.17%   60.91%   59.23%   58.26%   41.10%



"""

import sys
import math
import argparse
import csv
import binascii
import faulthandler
import json
try:
    import numpy as np  # For numpy see: http://numpy.org
except ImportError as e:
    print("This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.", file=sys.stderr)
    raise e
try:
    from scipy.sparse import coo_matrix
    report_cmat = True
except ImportError:
    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.", file=sys.stderr)
    report_cmat = False

IOBUFF = 100000000
sys.setrecursionlimit(1000000)
random_filler_value = 'ba8db6eb493e918dd0b9b7facc14a63caf0749d4510adbd022df4c13b8ba8f5f'
TRAINFILE = ['dataset.csv']
mapping = {'0': 0, '1': 1}
ignorelabels = []
ignorecolumns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '800', '801', '802', '804', '805', '806', '807', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894', '895', '896', '897', '898', '899', '900', '901', '902', '903', '904', '905', '906', '907', '908', '909', '910', '911', '913', '914', '915', '916', '917', '918', '919', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1029', '1030', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1088', '1089', '1090', '1091', '1092', '1093', '1094', '1095', '1096', '1097', '1098', '1099', '1100', '1101', '1102', '1103', '1104', '1105', '1106', '1107', '1108', '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '1120', '1121', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '1130', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '1140', '1141', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1151', '1152', '1153', '1154', '1155', '1156', '1157', '1158', '1159', '1160', '1161', '1162', '1163', '1164', '1165', '1166', '1167', '1168', '1169', '1170', '1171', '1172', '1173', '1174', '1175', '1176', '1177', '1178', '1179', '1180', '1181', '1182', '1183', '1184', '1185', '1186', '1187', '1188', '1189', '1190', '1191', '1192', '1193', '1194', '1195', '1196', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1204', '1205', '1206', '1207', '1208', '1209', '1210', '1211', '1212', '1213', '1214', '1215', '1216', '1217', '1218', '1219', '1220', '1221', '1222', '1223', '1224', '1225', '1226', '1227', '1228', '1229', '1230', '1231', '1232', '1233', '1234', '1235', '1236', '1237', '1238', '1239', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1248', '1249', '1250', '1251', '1252', '1253', '1254', '1255', '1256', '1257', '1258', '1259', '1260', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1277', '1278', '1279', '1280', '1281', '1282', '1283', '1284', '1285', '1286', '1287', '1288', '1289', '1290', '1291', '1292', '1293', '1294', '1295', '1296', '1297', '1298', '1299', '1300', '1301', '1302', '1304', '1305', '1306', '1307', '1308', '1309', '1310', '1311', '1312', '1313', '1314', '1315', '1316', '1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '1325', '1326', '1327', '1328', '1329', '1330', '1331', '1332', '1333', '1334', '1335', '1336', '1337', '1338', '1339', '1340', '1341', '1342', '1343', '1344', '1345', '1346', '1347', '1348', '1349', '1350', '1351', '1352', '1353', '1354', '1355', '1356', '1357', '1358', '1359', '1360', '1361', '1362', '1363', '1364', '1365', '1366', '1367', '1368', '1370', '1371', '1372', '1373', '1374', '1375', '1376', '1377', '1378', '1379', '1380', '1381', '1382', '1383', '1384', '1385', '1386', '1387', '1388', '1389', '1390', '1391', '1392', '1393', '1394', '1395', '1396', '1397', '1398', '1399', '1400', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1409', '1410', '1411', '1412', '1413', '1414', '1415', '1416', '1417', '1418', '1419', '1420', '1421', '1422', '1423', '1424', '1425', '1426', '1427', '1429', '1430', '1431', '1432', '1433', '1434', '1435', '1436', '1437', '1438', '1439', '1440', '1441', '1442', '1443', '1444', '1445', '1446', '1447', '1448', '1449', '1450', '1451', '1452', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '1460', '1461', '1462', '1463', '1465', '1466', '1467', '1468', '1469', '1470', '1471', '1473', '1474', '1475', '1476', '1477', '1478', '1479', '1480', '1481', '1482', '1483', '1484', '1485', '1486', '1487', '1488', '1489', '1490', '1491', '1492', '1493', '1494', '1495', '1496', '1497', '1498', '1499', '1500', '1501', '1502', '1503', '1504', '1505', '1506', '1507', '1508', '1509', '1510', '1511', '1512', '1513', '1514', '1515', '1516', '1517', '1518', '1519', '1520', '1521', '1522', '1523', '1524', '1525', '1526', '1527', '1528', '1529', '1530', '1531', '1532', '1533', '1534', '1535', '1536', '1537', '1538', '1539', '1540', '1541', '1542', '1543', '1544', '1545', '1546', '1547', '1548', '1549', '1550', '1551', '1552', '1553', '1554', '1555', '1556', '1557', '1558', '1559', '1560', '1561', '1562', '1563', '1564', '1565', '1566', '1568', '1569', '1570', '1571', '1572', '1573', '1574', '1575', '1576', '1577', '1578', '1579', '1580', '1581', '1582', '1583', '1584', '1585', '1586', '1587', '1588', '1589', '1590', '1591', '1592', '1593', '1594', '1595', '1596', '1597', '1598', '1599', '1600', '1601', '1602', '1603', '1604', '1605', '1606', '1607', '1608', '1609', '1610', '1611', '1612', '1614', '1615', '1616', '1617', '1618', '1619', '1620', '1622', '1623', '1624', '1625', '1627', '1629', '1630', '1631', '1632', '1633', '1634', '1635', '1636', '1637', '1638', '1639', '1640', '1641', '1642', '1643', '1644', '1645', '1646', '1647', '1648', '1649', '1650', '1651', '1652', '1653', '1654', '1655', '1656', '1657', '1658', '1659', '1660', '1661', '1662', '1663', '1664', '1665', '1666', '1667', '1668', '1669', '1670', '1671', '1672', '1673', '1674', '1675', '1676', '1677', '1678', '1679', '1680', '1681', '1682', '1683', '1684', '1685', '1686', '1687', '1688', '1689', '1690', '1692', '1693', '1694', '1695', '1696', '1697', '1698', '1699', '1700', '1701', '1702', '1703', '1704', '1705', '1706', '1707', '1708', '1709', '1710', '1711', '1712', '1713', '1714', '1715', '1716', '1717', '1718', '1719', '1720', '1721', '1722', '1723', '1724', '1725', '1726', '1727', '1728', '1729', '1730', '1731', '1732', '1733', '1734', '1735', '1736', '1737', '1738', '1739', '1740', '1741', '1742', '1743', '1744', '1745', '1746', '1747', '1748', '1749', '1750', '1751', '1752', '1753', '1754', '1755', '1756', '1757', '1758', '1759', '1760', '1761', '1762', '1763', '1764', '1765', '1766', '1767', '1768', '1769', '1770', '1771', '1772', '1773', '1774', '1775', '1776', '1777', '1778', '1779', '1780', '1781', '1782', '1783', '1784', '1785', '1786', '1787', '1788', '1789', '1790', '1791', '1792', '1793', '1794', '1795', '1796', '1797', '1798', '1799', '1800', '1801', '1802', '1803', '1804', '1805', '1806', '1807', '1808', '1809', '1810', '1811', '1812', '1813', '1814', '1816', '1817', '1818', '1819', '1820', '1821', '1822', '1823', '1824', '1825', '1826', '1827', '1828', '1829', '1830', '1831', '1832', '1833', '1834', '1835', '1836', '1837', '1838', '1839', '1840', '1841', '1842', '1843', '1844', '1845', '1846', '1847', '1848', '1849', '1850', '1851', '1852', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '1860', '1861', '1862', '1863', '1864', '1865', '1866', '1867', '1868', '1869', '1870', '1871', '1873', '1874', '1875', '1876', '1877', '1878', '1879', '1880', '1881', '1882', '1883', '1884', '1886', '1887', '1888', '1889', '1890', '1891', '1893', '1894', '1896', '1897', '1898', '1900', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1940', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1950', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030', '2031', '2032', '2033', '2034', '2035', '2036', '2038', '2039', '2040', '2041', '2043', '2044', '2045', '2046', '2047', '2048', '2049', '2050', '2051', '2052', '2053', '2054', '2055', '2056', '2057', '2058', '2059', '2060', '2061', '2062', '2063', '2065', '2066', '2067', '2068', '2069', '2070', '2071', '2072', '2073', '2074', '2075', '2076', '2077', '2078', '2079', '2080', '2081', '2082', '2083', '2084', '2085', '2086', '2087', '2088', '2089', '2090', '2091', '2092', '2093', '2094', '2095', '2096', '2097', '2098', '2099', '2100', '2101', '2102', '2103', '2104', '2105', '2106', '2107', '2108', '2109', '2110', '2111', '2112', '2113', '2114', '2115', '2116', '2117', '2118', '2119', '2120', '2122', '2123', '2125', '2126', '2127', '2128', '2129', '2130', '2131', '2132', '2133', '2134', '2135', '2136', '2137', '2138', '2139', '2140', '2141', '2142', '2143', '2144', '2146', '2147', '2148', '2149', '2150', '2151', '2152', '2153', '2155', '2156', '2157', '2158', '2159', '2160', '2161', '2162', '2163', '2164', '2165', '2166', '2167', '2168', '2169', '2170', '2171', '2172', '2173', '2174', '2175', '2176', '2177', '2178', '2179', '2180', '2181', '2182', '2183', '2184', '2185', '2186', '2187', '2188', '2189', '2190', '2191', '2192', '2193', '2194', '2195', '2196', '2197', '2198', '2199', '2200', '2202', '2203', '2204', '2205', '2206', '2207', '2208', '2209', '2211', '2212', '2213', '2214', '2215', '2216', '2217', '2218', '2219', '2220', '2221', '2222', '2223', '2224', '2225', '2226', '2227', '2228', '2229', '2230', '2231', '2232', '2233', '2234', '2236', '2237', '2238', '2239', '2240', '2241', '2242', '2243', '2244', '2245', '2246', '2247', '2248', '2249', '2250', '2251', '2252', '2253', '2254', '2255', '2256', '2257', '2258', '2259', '2260', '2262', '2263', '2264', '2265', '2266', '2267', '2268', '2269', '2270', '2271', '2272', '2273', '2274', '2275', '2276', '2277', '2278', '2279', '2280', '2281', '2282', '2283', '2284', '2285', '2286', '2287', '2288', '2289', '2290', '2291', '2292', '2293', '2294', '2295', '2297', '2298', '2299', '2300', '2301', '2302', '2303', '2304', '2305', '2306', '2307', '2308', '2309', '2310', '2311', '2312', '2313', '2314', '2315', '2316', '2317', '2318', '2319', '2320', '2321', '2322', '2323', '2324', '2325', '2326', '2327', '2328', '2329', '2330', '2331', '2332', '2333', '2334', '2335', '2336', '2337', '2338', '2339', '2340', '2341', '2342', '2343', '2344', '2345', '2346', '2347', '2348', '2349', '2350', '2351', '2352', '2353', '2354', '2355', '2356', '2357', '2358', '2359', '2360', '2361', '2362', '2363', '2364', '2365', '2366', '2367', '2368', '2369', '2370', '2371', '2372', '2374', '2375', '2377', '2378', '2379', '2380', '2381', '2382', '2383', '2384', '2385', '2386', '2387', '2388', '2389', '2390', '2391', '2392', '2393', '2394', '2395', '2396', '2397', '2398', '2399', '2400', '2401', '2402', '2403', '2404', '2405', '2406', '2407', '2408', '2409', '2410', '2411', '2412', '2413', '2414', '2415', '2416', '2417', '2418', '2419', '2420', '2421', '2422', '2423', '2424', '2425', '2426', '2427', '2428', '2429', '2430', '2431', '2432', '2433', '2434', '2435', '2436', '2437', '2438', '2439', '2440', '2441', '2442', '2443', '2444', '2445', '2446', '2447', '2448', '2449', '2450', '2451', '2452', '2453', '2454', '2455', '2456', '2457', '2458', '2459', '2460', '2461', '2462', '2463', '2464', '2465', '2466', '2467', '2468', '2469', '2470', '2471', '2472', '2473', '2474', '2475', '2476', '2477', '2478', '2479', '2480', '2481', '2482', '2483', '2484', '2485', '2486', '2487', '2488', '2489', '2490', '2491', '2492', '2493', '2494', '2495', '2496', '2497', '2498', '2499', '2500', '2501', '2502', '2503', '2504', '2505', '2506', '2507', '2508', '2509', '2510', '2511', '2512', '2513', '2514', '2515', '2516', '2517', '2518', '2519', '2520', '2521', '2522', '2523', '2524', '2525', '2526', '2527', '2528', '2529', '2530', '2531', '2532', '2533', '2534', '2535', '2536', '2537', '2538', '2539', '2540', '2541', '2542', '2543', '2544', '2545', '2546', '2547', '2548', '2549', '2550', '2551', '2552', '2553', '2554', '2555', '2556', '2557', '2558', '2559', '2560', '2561', '2562', '2563', '2564', '2565', '2566', '2567', '2568', '2569', '2570', '2571', '2572', '2573', '2574', '2575', '2576', '2577', '2578', '2579', '2580', '2581', '2582', '2583', '2584', '2585', '2586', '2587', '2588', '2589', '2590', '2591', '2592', '2593', '2594', '2595', '2596', '2597', '2598', '2599', '2600', '2601', '2602', '2603', '2604', '2605', '2606', '2607', '2608', '2609', '2610', '2611', '2612', '2613', '2614', '2615', '2616', '2617', '2618', '2619', '2620', '2621', '2622', '2623', '2624', '2625', '2626', '2627', '2628', '2629', '2630', '2631', '2632', '2633', '2634', '2635', '2636', '2637', '2638', '2639', '2640', '2641', '2642', '2643', '2644', '2645', '2646', '2647', '2648', '2649', '2650', '2651', '2652', '2653', '2654', '2655', '2656', '2657', '2658', '2659', '2660', '2661', '2662', '2663', '2664', '2665', '2666', '2667', '2668', '2669', '2670', '2671', '2672', '2673', '2674', '2675', '2676', '2677', '2678', '2679', '2680', '2681', '2682', '2683', '2684', '2685', '2686', '2687', '2688', '2689', '2690', '2691', '2692', '2693', '2694', '2695', '2696', '2697', '2698', '2699', '2700', '2701', '2702', '2703', '2704', '2705', '2706', '2707', '2708', '2709', '2711', '2712', '2713', '2714', '2715', '2716', '2717', '2718', '2719', '2720', '2721', '2722', '2723', '2724', '2725', '2726', '2727', '2728', '2729', '2730', '2731', '2732', '2733', '2734', '2735', '2736', '2737', '2738', '2739', '2740', '2741', '2742', '2743', '2744', '2745', '2746', '2747', '2748', '2749', '2750', '2751', '2752', '2753', '2754', '2755', '2756', '2757', '2758', '2759', '2760', '2761', '2762', '2763', '2764', '2765', '2766', '2767', '2768', '2769', '2770', '2771', '2772', '2773', '2774', '2775', '2776', '2777', '2778', '2779', '2780', '2781', '2782', '2783', '2784', '2785', '2786', '2787', '2788', '2789', '2790', '2791', '2792', '2793', '2794', '2795', '2796', '2797', '2798', '2799', '2800', '2801', '2802', '2803', '2804', '2805', '2806', '2807', '2808', '2809', '2810', '2811', '2812', '2813', '2814', '2815', '2816', '2817', '2818', '2819', '2820', '2821', '2822', '2823', '2824', '2825', '2826', '2827', '2828', '2829', '2830', '2831', '2832', '2833', '2834', '2835', '2836', '2837', '2838', '2839', '2840', '2841', '2842', '2843', '2844', '2845', '2846', '2847', '2848', '2849', '2850', '2851', '2852', '2853', '2854', '2855', '2856', '2857', '2858', '2859', '2860', '2861', '2862', '2863', '2864', '2865', '2866', '2867', '2868', '2869', '2870', '2872', '2873', '2874', '2875', '2876', '2877', '2878', '2879', '2880', '2881', '2882', '2883', '2884', '2885', '2886', '2887', '2888', '2889', '2890', '2891', '2892', '2893', '2894', '2895', '2896', '2897', '2898', '2899', '2900', '2901', '2902', '2903', '2904', '2905', '2906', '2907', '2908', '2909', '2910', '2911', '2912', '2913', '2914', '2915', '2916', '2917', '2918', '2919', '2920', '2921', '2922', '2923', '2924', '2925', '2926', '2927', '2928', '2929', '2930', '2931', '2932', '2933', '2934', '2935', '2936', '2937', '2938', '2939', '2940', '2941', '2942', '2943', '2944', '2945', '2946', '2947', '2948', '2949', '2950', '2951', '2953', '2954', '2956', '2957', '2958', '2959', '2960', '2961', '2962', '2963', '2964', '2965', '2966', '2967', '2968', '2969', '2970', '2971', '2972', '2973', '2974', '2975', '2976', '2977', '2978', '2979', '2980', '2981', '2982', '2983', '2984', '2985', '2986', '2987', '2988', '2989', '2990', '2991', '2992', '2993', '2994', '2995', '2996', '2997', '2998', '2999', '3000', '3001', '3002', '3003', '3004', '3005', '3006', '3007', '3008', '3009', '3010', '3011', '3012', '3013', '3014', '3015', '3016', '3017', '3018', '3019', '3020', '3021', '3022', '3023', '3024', '3025', '3026', '3027', '3028', '3029', '3030', '3031', '3032', '3033', '3034', '3035', '3036', '3037', '3038', '3039', '3041', '3042', '3043', '3044', '3045', '3046', '3047', '3048', '3049', '3051', '3052', '3053', '3054', '3055', '3056', '3057', '3058', '3059', '3060', '3061', '3062', '3063', '3064', '3065', '3066', '3067', '3068', '3069', '3070', '3071', '3072', '3074', '3075', '3076', '3077', '3078', '3079', '3080', '3081', '3082', '3083', '3084', '3085', '3086', '3087', '3088', '3089', '3090', '3091', '3092', '3093', '3094', '3095', '3096', '3097', '3098', '3099', '3100', '3101', '3102', '3103', '3104', '3105', '3106', '3107', '3108', '3109', '3110', '3111', '3112', '3113', '3114', '3115', '3116', '3117', '3118', '3119', '3120', '3121', '3122', '3123', '3124', '3125', '3126', '3127', '3128', '3129', '3130', '3131', '3132', '3133', '3134', '3135', '3136', '3137', '3138', '3139', '3140', '3141', '3142', '3143', '3144', '3145', '3146', '3147', '3148', '3149', '3150', '3151', '3152', '3154', '3155', '3156', '3158', '3159', '3160', '3161', '3162', '3163', '3164', '3165', '3166', '3167', '3168', '3169', '3170', '3171', '3172', '3173', '3174', '3175', '3176', '3177', '3178', '3179', '3180', '3181', '3182', '3183', '3184', '3185', '3186', '3187', '3188', '3189', '3190', '3191', '3192', '3193', '3194', '3195', '3196', '3197', '3198', '3199', '3200', '3201', '3202', '3203', '3204', '3205', '3206', '3207', '3208', '3209', '3210', '3211', '3212', '3213', '3214', '3215', '3216', '3217', '3218', '3219', '3220', '3221', '3222', '3223', '3224', '3225', '3226', '3227', '3228', '3229', '3230', '3231', '3232', '3233', '3234', '3235', '3236', '3237', '3238', '3239', '3240', '3241', '3242', '3243', '3244', '3245', '3246', '3247', '3248', '3249', '3250', '3251', '3252', '3253', '3254', '3255', '3256', '3257', '3258', '3259', '3260', '3261', '3262', '3263', '3264', '3265', '3266', '3267', '3268', '3269', '3270', '3271', '3272', '3273', '3274', '3275', '3276', '3277', '3279', '3280', '3281', '3282', '3283', '3284', '3285', '3287', '3288', '3289', '3290', '3291', '3292', '3293', '3294', '3295', '3296', '3297', '3298', '3299', '3300', '3301', '3302', '3303', '3304', '3305', '3306', '3307', '3308', '3309', '3311', '3312', '3313', '3314', '3315', '3316', '3317', '3318', '3319', '3320', '3321', '3322', '3323', '3324', '3325', '3326', '3327', '3328', '3329', '3330', '3331', '3332', '3333', '3334', '3335', '3336', '3337', '3338', '3339', '3340', '3341', '3342', '3343', '3344', '3345', '3346', '3347', '3348', '3349', '3350', '3351', '3352', '3353', '3354', '3355', '3356', '3357', '3358', '3359', '3360', '3361', '3362', '3363', '3364', '3365', '3366', '3367', '3368', '3370', '3371', '3372', '3373', '3374', '3375', '3376', '3377', '3378', '3379', '3380', '3381', '3382', '3383', '3384', '3385', '3386', '3387', '3388', '3389', '3390', '3391', '3392', '3393', '3394', '3395', '3396', '3397', '3398', '3399', '3400', '3401', '3402', '3403', '3404', '3405', '3406', '3407', '3408', '3409', '3410', '3411', '3412', '3413', '3414', '3415', '3416', '3417', '3418', '3419', '3420', '3421', '3422', '3423', '3424', '3425', '3426', '3427', '3428', '3429', '3430', '3431', '3432', '3433', '3434', '3435', '3436', '3438', '3439', '3440', '3441', '3442', '3443', '3444', '3445', '3446', '3447', '3448', '3449', '3450', '3451', '3452', '3454', '3455', '3456', '3457', '3458', '3459', '3460', '3461', '3462', '3463', '3464', '3465', '3466', '3467', '3468', '3469', '3470', '3471', '3472', '3473', '3474', '3475', '3476', '3477', '3478', '3479', '3480', '3481', '3482', '3483', '3484', '3485', '3486', '3487', '3488', '3489', '3490', '3491', '3492', '3493', '3494', '3495', '3496', '3497', '3498', '3499', '3500', '3501', '3502', '3503', '3504', '3505', '3506', '3507', '3508', '3509', '3510', '3511', '3512', '3513', '3515', '3516', '3517', '3518', '3519', '3520', '3521', '3522', '3523', '3524', '3525', '3526', '3527', '3528', '3529', '3530', '3531', '3532', '3534', '3535', '3537', '3538', '3539', '3540', '3541', '3542', '3543', '3544', '3545', '3546', '3547', '3548', '3549', '3550', '3551', '3552', '3553', '3554', '3555', '3556', '3557', '3558', '3559', '3560', '3561', '3562', '3563', '3564', '3565', '3566', '3567', '3568', '3570', '3571', '3572', '3573', '3574', '3575', '3576', '3577', '3578', '3579', '3580', '3581', '3582', '3583', '3584', '3585', '3586', '3587', '3588', '3589', '3590', '3592', '3593', '3594', '3596', '3597', '3598', '3599', '3600', '3601', '3602', '3603', '3604', '3605', '3606', '3607', '3608', '3609', '3610', '3611', '3612', '3613', '3614', '3615', '3616', '3617', '3618', '3619', '3620', '3621', '3622', '3623', '3624', '3625', '3626', '3627', '3628', '3629', '3630', '3631', '3633', '3634', '3635', '3636', '3637', '3638', '3639', '3640', '3641', '3642', '3643', '3644', '3645', '3646', '3647', '3648', '3649', '3650', '3651', '3652', '3653', '3654', '3655', '3656', '3657', '3658', '3659', '3660', '3661', '3662', '3663', '3664', '3665', '3666', '3667', '3668', '3669', '3671', '3672', '3673', '3674', '3675', '3676', '3677', '3678', '3679', '3680', '3681', '3682', '3683', '3684', '3685', '3686', '3687', '3688', '3689', '3690', '3691', '3692', '3693', '3694', '3696', '3697', '3698', '3699', '3700', '3701', '3702', '3703', '3704', '3705', '3706', '3707', '3708', '3709', '3710', '3711', '3712', '3713', '3714', '3715', '3716', '3717', '3718', '3719', '3720', '3721', '3722', '3723', '3724', '3725', '3726', '3727', '3728', '3729', '3730', '3731', '3732', '3733', '3734', '3735', '3736', '3737', '3738', '3739', '3740', '3741', '3742', '3743', '3744', '3745', '3746', '3747', '3748', '3749', '3750', '3751', '3752', '3753', '3754', '3756', '3757', '3758', '3759', '3760', '3761', '3762', '3763', '3764', '3765', '3766', '3767', '3768', '3769', '3770', '3771', '3772', '3773', '3774', '3775', '3776', '3777', '3778', '3779', '3780', '3781', '3782', '3783', '3784', '3785', '3786', '3787', '3788', '3789', '3790', '3791', '3792', '3793', '3794', '3795', '3796', '3797', '3798', '3799', '3801', '3802', '3803', '3804', '3805', '3806', '3807', '3808', '3809', '3810', '3811', '3812', '3813', '3814', '3815', '3816', '3817', '3818', '3819', '3820', '3821', '3822', '3823', '3824', '3825', '3826', '3827', '3828', '3829', '3830', '3831', '3832', '3833', '3834', '3835', '3836', '3837', '3838', '3839', '3840', '3841', '3842', '3843', '3844', '3845', '3846', '3847', '3848', '3849', '3850', '3851', '3852', '3853', '3854', '3855', '3856', '3857', '3858', '3859', '3860', '3861', '3862', '3863', '3864', '3865', '3866', '3867', '3868', '3869', '3870', '3871', '3872', '3873', '3874', '3875', '3876', '3877', '3878', '3879', '3880', '3881', '3882', '3883', '3884', '3885', '3886', '3887', '3888', '3889', '3890', '3891', '3892', '3893', '3894', '3895', '3896', '3897', '3898', '3899', '3900', '3901', '3902', '3903', '3904', '3905', '3906', '3907', '3908', '3909', '3910', '3911', '3912', '3913', '3915', '3916', '3917', '3918', '3919', '3920', '3921', '3922', '3923', '3924', '3925', '3926', '3927', '3928', '3929', '3930', '3931', '3932', '3933', '3934', '3935', '3936', '3937', '3938', '3939', '3940', '3941', '3942', '3943', '3944', '3946', '3947', '3948', '3949', '3950', '3951', '3952', '3953', '3954', '3955', '3956', '3957', '3958', '3959', '3960', '3961', '3962', '3963', '3964', '3965', '3966', '3967', '3968', '3969', '3970', '3971', '3972', '3973', '3974', '3975', '3976', '3977', '3978', '3979', '3980', '3981', '3982', '3983', '3984', '3985', '3986', '3987', '3988', '3989', '3990', '3991', '3992', '3993', '3994', '3995', '3996', '3997', '3998', '3999', '4000', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '4080', '4081', '4082', '4084', '4085', '4086', '4087', '4088', '4089', '4090', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '4100', '4101', '4102', '4103', '4104', '4105', '4106', '4107', '4108', '4109', '4110', '4111', '4112', '4113', '4114', '4115', '4116', '4117', '4118', '4119', '4120', '4121', '4122', '4123', '4124', '4125', '4126', '4127', '4128', '4129', '4130', '4131', '4132', '4133', '4134', '4135', '4136', '4137', '4138', '4139', '4140', '4141', '4142', '4143', '4144', '4145', '4146', '4147', '4148', '4149', '4150', '4151', '4152', '4153', '4154', '4155', '4156', '4157', '4158', '4159', '4160', '4161', '4162', '4163', '4164', '4165', '4166', '4167', '4168', '4169', '4170', '4171', '4172', '4173', '4174', '4175', '4176', '4177', '4178', '4179', '4180', '4181', '4182', '4183', '4184', '4185', '4186', '4187', '4188', '4189', '4190', '4191', '4192', '4193', '4194', '4195', '4196', '4197', '4198', '4199', '4200', '4201', '4202', '4203', '4204', '4205', '4206', '4207', '4208', '4209', '4210', '4211', '4212', '4213', '4214', '4215', '4216', '4217', '4218', '4219', '4220', '4221', '4222', '4223', '4224', '4225', '4226', '4227', '4228', '4229', '4230', '4231', '4232', '4233', '4234', '4235', '4236', '4237', '4238', '4239', '4240', '4241', '4242', '4243', '4244', '4245', '4246', '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254', '4255', '4256', '4257', '4258', '4259', '4260', '4261', '4262', '4263', '4264', '4265', '4266', '4267', '4268', '4269', '4270', '4271', '4273', '4274', '4275', '4276', '4277', '4278', '4279', '4280', '4281', '4282', '4283', '4284', '4285', '4286', '4287', '4288', '4289', '4290', '4291', '4292', '4293', '4294', '4295', '4296', '4297', '4298', '4299', '4300', '4301', '4302', '4303', '4304', '4305', '4306', '4307', '4308', '4309', '4310', '4311', '4312', '4313', '4314', '4315', '4316', '4317', '4318', '4319', '4320', '4321', '4322', '4323', '4324', '4325', '4326', '4327', '4328', '4329', '4330', '4331', '4332', '4333', '4334', '4335', '4336', '4337', '4338', '4339', '4340', '4341', '4342', '4343', '4344', '4345', '4346', '4347', '4348', '4349', '4350', '4351', '4352', '4353', '4354', '4355', '4356', '4357', '4358', '4359', '4360', '4361', '4362', '4363', '4364', '4365', '4366', '4367', '4368', '4369', '4370', '4371', '4372', '4373', '4374', '4375', '4376', '4377', '4378', '4379', '4380', '4381', '4382', '4383', '4384', '4385', '4386', '4387', '4388', '4389', '4390', '4391', '4392', '4393', '4394', '4395', '4396', '4397', '4398', '4399', '4400', '4401', '4402', '4403', '4404', '4405', '4406', '4407', '4408', '4409', '4410', '4411', '4412', '4413', '4414', '4415', '4416', '4417', '4418', '4419', '4420', '4421', '4422', '4423', '4424', '4425', '4426', '4427', '4428', '4429', '4430', '4431', '4432', '4433', '4434', '4435', '4436', '4437', '4438', '4439', '4440', '4441', '4442', '4443', '4444', '4445', '4446', '4447', '4448', '4449', '4450', '4451', '4452', '4453', '4454', '4455', '4456', '4457', '4458', '4459', '4460', '4461', '4462', '4463', '4464', '4465', '4466', '4467', '4468', '4469', '4470', '4471', '4472', '4473', '4474', '4475', '4476', '4477', '4478', '4479', '4480', '4481', '4482', '4483', '4484', '4485', '4486', '4487', '4488', '4489', '4490', '4491', '4492', '4493', '4494', '4495', '4496', '4497', '4498', '4499', '4500', '4501', '4502', '4503', '4504', '4505', '4506', '4507', '4508', '4509', '4510', '4511', '4512', '4514', '4515', '4516', '4517', '4518', '4519', '4520', '4521', '4522', '4523', '4524', '4525', '4526', '4527', '4528', '4529', '4531', '4532', '4533', '4534', '4535', '4536', '4537', '4538', '4539', '4540', '4541', '4542', '4543', '4544', '4545', '4546', '4547', '4548', '4549', '4550', '4551', '4552', '4553', '4554', '4555', '4556', '4557', '4558', '4559', '4560', '4561', '4562', '4563', '4564', '4565', '4566', '4567', '4568', '4569', '4570', '4571', '4572', '4573', '4574', '4575', '4576', '4577', '4578', '4579', '4580', '4581', '4582', '4583', '4584', '4585', '4586', '4587', '4588', '4589', '4590', '4591', '4592', '4593', '4594', '4595', '4596', '4597', '4598', '4599', '4600', '4601', '4602', '4603', '4604', '4605', '4606', '4607', '4608', '4609', '4610', '4611', '4612', '4613', '4614', '4615', '4616', '4617', '4618', '4619', '4620', '4621', '4622', '4623', '4624', '4625', '4626', '4627', '4628', '4629', '4630', '4631', '4632', '4633', '4634', '4635', '4636', '4637', '4638', '4639', '4640', '4641', '4642', '4643', '4644', '4645', '4646', '4647', '4648', '4649', '4650', '4651', '4652', '4653', '4654', '4655', '4656', '4657', '4658', '4659', '4660', '4661', '4662', '4663', '4664', '4665', '4666', '4667', '4668', '4669', '4670', '4671', '4672', '4673', '4674', '4675', '4676', '4677', '4678', '4679', '4680', '4681', '4682', '4683', '4684', '4685', '4686', '4687', '4688', '4689', '4690', '4691', '4692', '4693', '4694', '4695', '4696', '4697', '4699', '4700', '4701', '4702', '4703', '4704', '4705', '4706', '4707', '4708', '4709', '4710', '4711', '4713', '4714', '4716', '4717', '4718', '4719', '4720', '4721', '4722', '4723', '4724', '4725', '4726', '4727', '4728', '4729', '4730', '4731', '4732', '4733', '4734', '4735', '4736', '4737', '4738', '4739', '4740', '4741', '4742', '4743', '4744', '4745', '4746', '4747', '4748', '4749', '4750', '4751', '4752', '4753', '4754', '4755', '4756', '4757', '4758', '4759', '4760', '4761', '4762', '4763', '4764', '4765', '4766', '4767', '4768', '4769', '4770', '4771', '4772', '4773', '4774', '4775', '4776', '4777', '4778', '4779', '4780', '4781', '4782', '4783', '4784', '4785', '4786', '4787', '4788', '4789', '4790', '4791', '4792', '4793', '4794', '4795', '4796', '4797', '4798', '4799', '4800', '4801', '4802', '4803', '4804', '4805', '4806', '4807', '4808', '4809', '4810', '4811', '4812', '4813', '4814', '4815', '4816', '4817', '4818', '4819', '4820', '4821', '4822', '4823', '4824', '4825', '4826', '4827', '4828', '4829', '4830', '4831', '4832', '4833', '4834', '4835', '4836', '4837', '4838', '4839', '4840', '4841', '4842', '4843', '4844', '4845', '4846', '4847', '4848', '4850', '4851', '4852', '4853', '4854', '4855', '4856', '4857', '4858', '4859', '4860', '4861', '4862', '4863', '4864', '4865', '4866', '4867', '4868', '4869', '4870', '4871', '4872', '4873', '4874', '4875', '4876', '4877', '4878', '4879', '4880', '4881', '4882', '4883', '4884', '4885', '4887', '4888', '4889', '4890', '4891', '4892', '4893', '4894', '4895', '4896', '4897', '4898', '4899', '4900', '4901', '4902', '4903', '4904', '4905', '4906', '4907', '4908', '4909', '4910', '4911', '4912', '4913', '4914', '4915', '4916', '4917', '4918', '4919', '4920', '4921', '4922', '4923', '4924', '4925', '4926', '4927', '4928', '4929', '4930', '4931', '4932', '4933', '4934', '4936', '4937', '4938', '4939', '4940', '4941', '4942', '4943', '4944', '4945', '4946', '4947', '4948', '4949', '4950', '4951', '4952', '4953', '4954', '4955', '4956', '4957', '4958', '4959', '4960', '4961', '4962', '4963', '4964', '4965', '4966', '4967', '4968', '4969', '4970', '4971', '4972', '4973', '4974', '4975', '4976', '4977', '4978', '4979', '4980', '4981', '4982', '4983', '4984', '4985', '4986', '4987', '4988', '4989', '4990', '4991', '4992', '4993', '4994', '4995', '4996', '4997', '4998', '4999', '5000', '5001', '5002', '5003', '5004', '5005', '5006', '5007', '5008', '5009', '5010', '5011', '5012', '5013', '5014', '5015', '5016', '5017', '5018', '5019', '5021', '5022', '5023', '5024', '5025', '5026', '5027', '5028', '5029', '5030', '5031', '5032', '5033', '5034', '5035', '5036', '5037', '5038', '5039', '5040', '5041', '5042', '5043', '5044', '5045', '5046', '5047', '5048', '5049', '5050', '5051', '5052', '5053', '5054', '5055', '5056', '5057', '5058', '5059', '5060', '5061', '5062', '5063', '5064', '5065', '5066', '5067', '5068', '5069', '5070', '5071', '5072', '5073', '5074', '5075', '5076', '5077', '5078', '5079', '5080', '5081', '5082', '5083', '5084', '5085', '5086', '5087', '5088', '5089', '5090', '5091', '5092', '5093', '5094', '5095', '5096', '5097', '5098', '5099', '5100', '5101', '5102', '5104', '5105', '5106', '5107', '5108', '5109', '5110', '5111', '5112', '5113', '5114', '5115', '5116', '5117', '5118', '5119', '5120', '5121', '5122', '5123', '5124', '5125', '5126', '5127', '5128', '5129', '5130', '5131', '5132', '5133', '5134', '5135', '5136', '5137', '5138', '5139', '5140', '5141', '5142', '5143', '5144', '5145', '5146', '5147', '5148', '5149', '5150', '5151', '5152', '5153', '5154', '5155', '5156', '5157', '5158', '5159', '5160', '5161', '5162', '5163', '5164', '5165', '5166', '5167', '5168', '5169', '5170', '5171', '5172', '5173', '5174', '5175', '5176', '5177', '5178', '5179', '5180', '5181', '5182', '5183', '5185', '5186', '5187', '5188', '5189', '5190', '5191', '5192', '5193', '5194', '5195', '5196', '5197', '5198', '5199', '5200', '5201', '5202', '5203', '5204', '5205', '5206', '5207', '5208', '5209', '5210', '5211', '5212', '5213', '5214', '5215', '5216', '5217', '5218', '5219', '5220', '5221', '5222', '5223', '5224', '5225', '5226', '5227', '5228', '5229', '5230', '5231', '5232', '5233', '5234', '5235', '5236', '5237', '5238', '5239', '5240', '5241', '5242', '5243', '5244', '5245', '5246', '5247', '5248', '5249', '5250', '5251', '5252', '5253', '5254', '5255', '5256', '5257', '5258', '5259', '5260', '5261', '5262', '5263', '5264', '5265', '5266', '5267', '5268', '5269', '5270', '5271', '5272', '5273', '5274', '5275', '5276', '5277', '5278', '5279', '5280', '5281', '5282', '5283', '5284', '5285', '5286', '5287', '5288', '5289', '5290', '5291', '5292', '5293', '5294', '5295', '5296', '5297', '5298', '5299', '5300', '5301', '5302', '5303', '5304', '5305', '5306', '5307', '5308', '5309', '5311', '5312', '5313', '5314', '5315', '5316', '5317', '5318', '5319', '5320', '5321', '5322', '5323', '5324', '5325', '5326', '5327', '5328', '5329', '5330', '5331', '5332', '5333', '5334', '5335', '5336', '5337', '5338', '5339', '5340', '5341', '5342', '5343', '5344', '5345', '5346', '5347', '5348', '5349', '5350', '5351', '5352', '5353', '5354', '5355', '5356', '5357', '5358', '5359', '5360', '5361', '5362', '5363', '5364', '5365', '5366', '5367', '5368', '5369', '5370', '5371', '5372', '5373', '5374', '5375', '5376', '5377', '5378', '5379', '5380', '5381', '5382', '5383', '5384', '5385', '5386', '5387', '5388', '5389', '5390', '5391', '5392', '5393', '5394', '5395', '5396', '5397', '5398', '5399', '5400', '5401', '5402', '5403', '5404', '5405', '5406', '5407', '5408', '5409', '5410', '5411', '5412', '5413', '5414', '5415', '5416', '5417', '5418', '5419', '5420', '5421', '5422', '5423', '5424', '5425', '5426', '5427', '5428', '5429', '5430', '5431', '5432', '5433', '5434', '5435', '5436', '5437', '5438', '5439', '5440', '5441', '5442', '5443', '5444', '5445', '5446', '5447', '5448', '5449', '5450', '5451', '5452', '5453', '5454', '5455', '5456', '5457', '5458', '5459', '5460', '5461', '5462', '5463', '5464', '5465', '5467', '5468', '5469', '5470', '5471', '5472', '5473', '5474', '5475', '5476', '5477', '5478', '5479', '5480', '5481', '5482', '5483', '5484', '5485', '5486', '5487', '5488', '5489', '5490', '5491', '5492', '5493', '5494', '5495', '5496', '5497', '5498', '5499', '5500', '5501', '5502', '5503', '5504', '5505', '5506', '5507', '5508', '5509', '5510', '5511', '5512', '5513', '5514', '5515', '5516', '5517', '5518', '5519', '5520', '5522', '5523', '5524', '5525', '5526', '5527', '5528', '5529', '5530', '5531', '5532', '5533', '5534', '5535', '5536', '5537', '5538', '5539', '5540', '5541', '5542', '5543', '5544', '5545', '5546', '5547', '5549', '5550', '5551', '5552', '5553', '5554', '5555', '5556', '5557', '5558', '5559', '5560', '5561', '5562', '5563', '5564', '5565', '5566', '5567', '5568', '5569', '5570', '5571', '5572', '5573', '5574', '5575', '5576', '5577', '5578', '5579', '5580', '5581', '5582', '5583', '5584', '5585', '5586', '5587', '5588', '5589', '5590', '5591', '5592', '5593', '5594', '5595', '5596', '5597', '5598', '5599', '5600', '5601', '5602', '5603', '5604', '5605', '5606', '5607', '5608', '5609', '5610', '5611', '5612', '5613', '5614', '5615', '5616', '5617', '5618', '5619', '5620', '5621', '5622', '5623', '5624', '5625', '5626', '5627', '5628', '5629', '5630', '5631', '5632', '5633', '5634', '5635', '5636', '5637', '5638', '5639', '5640', '5641', '5642', '5643', '5644', '5645', '5646', '5647', '5648', '5649', '5650', '5651', '5652', '5653', '5654', '5655', '5656', '5657', '5658', '5659', '5660', '5661', '5662', '5663', '5664', '5665', '5666', '5667', '5668', '5669', '5670', '5671', '5672', '5673', '5674', '5675', '5676', '5677', '5678', '5679', '5680', '5681', '5682', '5683', '5684', '5685', '5686', '5687', '5688', '5689', '5690', '5692', '5693', '5694', '5695', '5696', '5697', '5698', '5699', '5700', '5701', '5702', '5703', '5704', '5705', '5706', '5707', '5708', '5709', '5710', '5711', '5712', '5713', '5714', '5715', '5716', '5717', '5718', '5719', '5720', '5721', '5722', '5723', '5724', '5725', '5726', '5727', '5728', '5729', '5730', '5731', '5732', '5734', '5735', '5736', '5737', '5738', '5739', '5740', '5741', '5742', '5743', '5744', '5745', '5746', '5747', '5748', '5749', '5750', '5751', '5752', '5753', '5754', '5755', '5756', '5757', '5758', '5759', '5760', '5761', '5762', '5763', '5764', '5765', '5766', '5767', '5768', '5769', '5770', '5771', '5772', '5773', '5774', '5775', '5776', '5777', '5778', '5779', '5780', '5781', '5782', '5783', '5784', '5785', '5786', '5787', '5788', '5789', '5790', '5791', '5792', '5793', '5794', '5795', '5796', '5797', '5798', '5799', '5800', '5801', '5802', '5803', '5804', '5805', '5806', '5807', '5808', '5809', '5810', '5811', '5812', '5813', '5814', '5815', '5816', '5817', '5818', '5819', '5820', '5821', '5822', '5823', '5824', '5825', '5826', '5827', '5828', '5829', '5830', '5831', '5832', '5833', '5834', '5835', '5836', '5837', '5838', '5839', '5840', '5841', '5842', '5843', '5844', '5845', '5846', '5847', '5848', '5849', '5850', '5851', '5852', '5853', '5854', '5855', '5856', '5857', '5858', '5859', '5860', '5861', '5862', '5863', '5864', '5865', '5866', '5867', '5868', '5869', '5870', '5871', '5872', '5873', '5874', '5875', '5876', '5877', '5878', '5879', '5880', '5881', '5882', '5883', '5884', '5885', '5886', '5887', '5888', '5889', '5890', '5891', '5892', '5893', '5894', '5895', '5896', '5897', '5898', '5899', '5900', '5901', '5902', '5903', '5904', '5905', '5906', '5907', '5908', '5909', '5910', '5911', '5912', '5913', '5914', '5915', '5916', '5917', '5918', '5919', '5920', '5921', '5922', '5923', '5924', '5925', '5926', '5927', '5928', '5929', '5930', '5931', '5932', '5933', '5934', '5935', '5936', '5937', '5938', '5939', '5940', '5941', '5942', '5943', '5944', '5945', '5946', '5947', '5948', '5949', '5950', '5951', '5952', '5953', '5954', '5955', '5956', '5957', '5958', '5959', '5960', '5961', '5962', '5963', '5964', '5965', '5966', '5967', '5968', '5969', '5970', '5971', '5972', '5973', '5974', '5975', '5976', '5977', '5978', '5979', '5980', '5981', '5982', '5983', '5984', '5985', '5986', '5987', '5988', '5989', '5990', '5991', '5992', '5993', '5994', '5995', '5996', '5997', '5998', '5999', '6000', '6001', '6002', '6003', '6004', '6005', '6006', '6007', '6008', '6009', '6010', '6011', '6012', '6013', '6014', '6015', '6016', '6017', '6018', '6019', '6020', '6021', '6022', '6023', '6024', '6025', '6026', '6027', '6028', '6029', '6030', '6031', '6032', '6033', '6034', '6035', '6036', '6037', '6038', '6039', '6040', '6041', '6042', '6043', '6044', '6045', '6046', '6047', '6048', '6049', '6050', '6051', '6052', '6053', '6054', '6055', '6056', '6057', '6058', '6059', '6060', '6061', '6062', '6063', '6064', '6065', '6066', '6067', '6068', '6069', '6071', '6072', '6073', '6074', '6075', '6076', '6077', '6078', '6079', '6080', '6081', '6083', '6084', '6085', '6086', '6087', '6088', '6089', '6090', '6091', '6092', '6093', '6094', '6095', '6096', '6097', '6098', '6099', '6100', '6101', '6102', '6103', '6104', '6105', '6106', '6107', '6108', '6109', '6110', '6111', '6112', '6113', '6114', '6115', '6116', '6117', '6118', '6119', '6120', '6121', '6122', '6123', '6124', '6125', '6126', '6127', '6128', '6129', '6130', '6131', '6132', '6133', '6134', '6135', '6136', '6137', '6138', '6139', '6140', '6141', '6142', '6143', '6144', '6145', '6146', '6147', '6148', '6149', '6150', '6151', '6152', '6153', '6154', '6155', '6156', '6157', '6158', '6159', '6160', '6161', '6162', '6163', '6164', '6165', '6166', '6167', '6168', '6169', '6170', '6171', '6172', '6173', '6174', '6175', '6176', '6177', '6178', '6179', '6180', '6181', '6182', '6183', '6184', '6185', '6186', '6187', '6188', '6189', '6190', '6191', '6192', '6193', '6194', '6195', '6196', '6197', '6198', '6199', '6200', '6201', '6202', '6203', '6204', '6205', '6206', '6207', '6208', '6209', '6210', '6211', '6212', '6213', '6214', '6215', '6216', '6217', '6218', '6219', '6220', '6221', '6222', '6223', '6224', '6225', '6226', '6227', '6228', '6229', '6230', '6231', '6232', '6233', '6234', '6235', '6236', '6237', '6238', '6239', '6240', '6241', '6242', '6243', '6244', '6245', '6246', '6247', '6248', '6249', '6250', '6251', '6252', '6253', '6254', '6255', '6256', '6257', '6258', '6259', '6260', '6261', '6262', '6263', '6264', '6265', '6266', '6267', '6268', '6269', '6270', '6271', '6272', '6273', '6274', '6275', '6276', '6277', '6278', '6279', '6280', '6281', '6282', '6283', '6284', '6285', '6286', '6287', '6288', '6289', '6290', '6291', '6292', '6293', '6294', '6295', '6296', '6297', '6298', '6299', '6300', '6301', '6302', '6303', '6304', '6305', '6306', '6307', '6308', '6309', '6310', '6311', '6312', '6313', '6314', '6315', '6316', '6317', '6318', '6319', '6320', '6321', '6322', '6323', '6324', '6325', '6326', '6327', '6328', '6329', '6330', '6331', '6332', '6333', '6334', '6335', '6336', '6337', '6338', '6339', '6340', '6341', '6342', '6343', '6344', '6345', '6346', '6347', '6348', '6349', '6350', '6351', '6352', '6353', '6354', '6355', '6356', '6357', '6358', '6359', '6360', '6361', '6362', '6363', '6364', '6365', '6366', '6367', '6368', '6369', '6370', '6371', '6372', '6373', '6374', '6375', '6376', '6377', '6378', '6379', '6380', '6381', '6382', '6383', '6384', '6385', '6386', '6387', '6388', '6389', '6390', '6391', '6392', '6393', '6394', '6395', '6396', '6397', '6399', '6400', '6401', '6402', '6403', '6404', '6405', '6406', '6407', '6408', '6409', '6410', '6411', '6412', '6413', '6414', '6415', '6416', '6417', '6418', '6419', '6420', '6421', '6422', '6423', '6424', '6425', '6426', '6427', '6428', '6429', '6430', '6431', '6432', '6433', '6434', '6435', '6436', '6437', '6438', '6439', '6440', '6441', '6442', '6443', '6444', '6445', '6446', '6447', '6448', '6449', '6450', '6451', '6452', '6453', '6454', '6455', '6456', '6457', '6458', '6459', '6460', '6461', '6462', '6463', '6464', '6465', '6466', '6467', '6468', '6469', '6470', '6471', '6472', '6473', '6474', '6475', '6476', '6477', '6478', '6479', '6480', '6481', '6482', '6483', '6484', '6485', '6486', '6487', '6488', '6489', '6490', '6491', '6492', '6493', '6494', '6495', '6496', '6497', '6498', '6499', '6500', '6501', '6502', '6503', '6504', '6505', '6506', '6507', '6508', '6509', '6510', '6511', '6512', '6513', '6514', '6515', '6516', '6517', '6518', '6519', '6520', '6521', '6522', '6523', '6524', '6525', '6526', '6527', '6528', '6529', '6530', '6531', '6532', '6533', '6534', '6535', '6536', '6537', '6538', '6539', '6540', '6541', '6542', '6543', '6544', '6545', '6546', '6547', '6548', '6549', '6550', '6551', '6552', '6553', '6554', '6556', '6557', '6558', '6559', '6560', '6561', '6562', '6563', '6564', '6565', '6566', '6567', '6568', '6569', '6570', '6571', '6572', '6573', '6574', '6575', '6576', '6577', '6578', '6579', '6580', '6581', '6582', '6583', '6584', '6585', '6586', '6587', '6588', '6589', '6590', '6591', '6592', '6593', '6594', '6595', '6596', '6597', '6598', '6599', '6600', '6601', '6602', '6604', '6605', '6606', '6607', '6608', '6609', '6610', '6611', '6612', '6613', '6614', '6615', '6616', '6617', '6618', '6619', '6620', '6621', '6622', '6623', '6624', '6625', '6626', '6627', '6628', '6629', '6630', '6631', '6632', '6633', '6634', '6635', '6636', '6637', '6638', '6639', '6640', '6641', '6642', '6643', '6644', '6645', '6646', '6647', '6648', '6649', '6650', '6651', '6652', '6653', '6654', '6655', '6656', '6657', '6658', '6659', '6660', '6661', '6662', '6663', '6664', '6665', '6666', '6667', '6668', '6669', '6670', '6671', '6672', '6673', '6674', '6675', '6676', '6677', '6678', '6679', '6680', '6681', '6682', '6683', '6684', '6685', '6686', '6687', '6688', '6689', '6690', '6691', '6692', '6693', '6694', '6695', '6696', '6697', '6698', '6699', '6700', '6701', '6702', '6703', '6704', '6705', '6706', '6707', '6708', '6709', '6710', '6711', '6712', '6713', '6714', '6715', '6716', '6717', '6718', '6719', '6720', '6721', '6722', '6723', '6724', '6725', '6726', '6727', '6728', '6729', '6730', '6731', '6732', '6733', '6734', '6735', '6736', '6737', '6738', '6739', '6740', '6741', '6742', '6743', '6744', '6745', '6746', '6747', '6748', '6749', '6750', '6751', '6752', '6753', '6754', '6755', '6756', '6757', '6758', '6759', '6761', '6762', '6763', '6764', '6765', '6766', '6767', '6768', '6769', '6771', '6772', '6773', '6774', '6776', '6777', '6778', '6779', '6780', '6781', '6782', '6783', '6784', '6785', '6786', '6787', '6788', '6789', '6790', '6791', '6792', '6793', '6794', '6795', '6796', '6797', '6798', '6799', '6800', '6801', '6802', '6803', '6804', '6805', '6806', '6807', '6808', '6809', '6810', '6811', '6812', '6813', '6814', '6815', '6816', '6817', '6818', '6819', '6820', '6821', '6822', '6823', '6824', '6825', '6826', '6827', '6828', '6829', '6830', '6831', '6832', '6833', '6834', '6835', '6836', '6837', '6838', '6839', '6840', '6841', '6842', '6843', '6844', '6845', '6846', '6847', '6848', '6849', '6850', '6851', '6852', '6853', '6854', '6855', '6856', '6857', '6858', '6859', '6860', '6861', '6862', '6863', '6864', '6865', '6866', '6867', '6868', '6869', '6870', '6871', '6872', '6873', '6874', '6875', '6876', '6877', '6878', '6879', '6880', '6881', '6882', '6883', '6884', '6885', '6886', '6887', '6888', '6889', '6890', '6891', '6892', '6893', '6894', '6895', '6896', '6897', '6898', '6899', '6900', '6901', '6902', '6903', '6904', '6905', '6906', '6907', '6908', '6909', '6910', '6911', '6912', '6913', '6914', '6915', '6916', '6917', '6919', '6920', '6921', '6922', '6923', '6924', '6925', '6926', '6927', '6928', '6929', '6930', '6931', '6932', '6933', '6934', '6935', '6936', '6937', '6938', '6939', '6940', '6941', '6942', '6943', '6944', '6945', '6946', '6947', '6948', '6949', '6950', '6951', '6952', '6953', '6954', '6955', '6956', '6957', '6958', '6959', '6960', '6961', '6962', '6963', '6964', '6965', '6966', '6967', '6968', '6969', '6970', '6971', '6972', '6973', '6974', '6975', '6976', '6977', '6978', '6979', '6980', '6981', '6982', '6983', '6984', '6985', '6986', '6987', '6988', '6989', '6990', '6991', '6992', '6993', '6994', '6995', '6996', '6997', '6998', '6999', '7000', '7001', '7002', '7003', '7004', '7005', '7006', '7007', '7008', '7009', '7010', '7011', '7012', '7013', '7014', '7015', '7016', '7017', '7018', '7019', '7020', '7021', '7022', '7023', '7024', '7025', '7026', '7027', '7028', '7029', '7030', '7031', '7032', '7034', '7035', '7036', '7037', '7038', '7040', '7041', '7042', '7043', '7044', '7045', '7046', '7047', '7048', '7049', '7050', '7051', '7052', '7053', '7054', '7055']
target = '7056'
target_column = 7056
important_idxs = [72, 392, 803, 912, 971, 1028, 1038, 1142, 1303, 1369, 1428, 1464, 1472, 1567, 1613, 1621, 1626, 1628, 1691, 1815, 1872, 1885, 1892, 1895, 1899, 1951, 1984, 2037, 2042, 2064, 2121, 2124, 2145, 2154, 2201, 2210, 2235, 2261, 2296, 2373, 2376, 2710, 2871, 2952, 2955, 3040, 3050, 3073, 3153, 3157, 3278, 3286, 3310, 3369, 3437, 3453, 3514, 3533, 3536, 3569, 3591, 3595, 3632, 3670, 3695, 3755, 3800, 3914, 3945, 4083, 4091, 4272, 4513, 4530, 4698, 4712, 4715, 4849, 4886, 4935, 5020, 5103, 5184, 5310, 5466, 5521, 5548, 5691, 5733, 6070, 6082, 6398, 6555, 6603, 6760, 6770, 6775, 6918, 7033, 7039]
expected_feature_cols = 7056
classifier_type = 'NN'
num_attr = 7056
n_classes = 2
model_cap = 172
w_h = np.array([[-0.057966072112321854, 0.16834545135498047, -0.39948558807373047, -0.08928626030683517, -0.15668201446533203, 0.02113504894077778, 0.11256695538759232, 0.27481403946876526, -0.09889619797468185, 0.013182180933654308, -0.2131509929895401, 0.016960710287094116, -0.1520974487066269, -0.18365991115570068, 0.010392213240265846, -0.14121010899543762, 0.09310868382453918], [-0.01255713403224945, -0.4004282057285309, -0.44939473271369934, 0.23090077936649323, 0.4542904794216156, 0.022710688412189484, 0.26713472604751587, -0.05889376997947693, 0.11988354474306107, -0.009301894344389439, -0.5479718446731567, -0.3573046922683716, 0.0890912339091301, -0.008467812091112137, 0.09563587605953217, -0.2868482768535614, 0.0508386604487896], [-0.00997666921466589, -0.20411643385887146, -0.24646151065826416, 0.12189080566167831, 0.24482639133930206, 0.010306495241820812, 0.14189977943897247, -0.0286567360162735, 0.05639654025435448, -0.004767111036926508, -0.2999299168586731, -0.18356133997440338, 0.046808190643787384, -0.0116271385923028, 0.05663364753127098, -0.15536780655384064, 0.029461737722158432], [0.2967458963394165, 0.311654657125473, -0.3032064735889435, 0.005876481533050537, -0.022274263203144073, 0.19786401093006134, 0.023451710119843483, 0.040851276367902756, -0.1418459415435791, -0.0843530222773552, 0.08587919920682907, -0.2849040925502777, -0.09434209764003754, 0.12735657393932343, 0.10637687891721725, -0.14121845364570618, 0.15855833888053894], [0.293506920337677, 0.3482622802257538, -0.35712721943855286, -0.03297066316008568, -0.014232205227017403, 0.23080182075500488, 0.07766364514827728, 0.16240619122982025, -0.13764376938343048, -0.15749064087867737, 0.13180705904960632, -0.34049201011657715, -0.16029754281044006, 0.1441303789615631, 0.07007701694965363, -0.2284272313117981, 0.1718003749847412], [0.03546729311347008, 0.025610992684960365, -0.14345139265060425, 0.06034659594297409, -0.2737552523612976, 0.12293556332588196, 0.14567150175571442, 0.35491281747817993, 0.1458122283220291, -0.31628116965293884, -0.247757226228714, 0.21959903836250305, 0.04343928024172783, -0.3410768508911133, -0.32408925890922546, 0.46738186478614807, 0.25664442777633667], [-0.04437735304236412, 0.18912860751152039, -0.47069796919822693, -0.13169613480567932, -0.3234657049179077, 0.0021046623587608337, 0.025537047535181046, 0.34807395935058594, -0.1393565535545349, 0.1940932720899582, -0.22375720739364624, 0.10150596499443054, -0.14584188163280487, -0.3142673671245575, -0.04864509403705597, -0.11033100634813309, 0.09987161308526993], [0.2895215153694153, 0.1569569706916809, -0.18107934296131134, 0.16495771706104279, -0.009549923241138458, -0.017859352752566338, -0.13867062330245972, -0.2863847613334656, -0.16604499518871307, 0.10625920444726944, -0.205898255109787, -0.09269288927316666, 0.0977325588464737, 0.10728084295988083, 0.2612493932247162, 0.18534068763256073, -0.009639430791139603], [0.16880524158477783, -0.0859706848859787, 0.018984349444508553, 0.3864082396030426, 0.3421260714530945, 0.1538662612438202, 0.3423338234424591, 0.12135738134384155, -0.5136494636535645, 0.13852424919605255, 0.12350504845380783, -0.22520841658115387, -0.03532060980796814, -0.26347529888153076, -0.27444708347320557, -0.36445507407188416, -0.17432008683681488]])
b_h = np.array([0.5421903729438782, 0.011994916945695877, -0.11620258539915085, 0.49518707394599915, -0.07536504417657852, 0.07556258887052536, -0.43516573309898376, -0.4256799817085266, -0.29283028841018677])
w_o = np.array([[0.09319409728050232, 0.2440752238035202, -0.44948676228523254, 0.23331424593925476, -0.19185103476047516, -0.0033768184948712587, -0.0551246777176857, -0.04358839988708496, -0.008316623978316784]])
b_o = np.array(-0.22582976520061493)


class PredictorError(Exception):

    def __init__(self, msg, code):
        self.msg = msg
        self.code = code

    def __str__(self):
        return self.msg
def __transform(X):
    mean = np.array([107.29270833333334, 119.2375, 125.85729166666667, 114.86041666666667, 125.778125, 122.08958333333334, 126.66354166666666, 127.94270833333333, 129.33333333333334, 127.22604166666666, 111.47916666666667, 132.759375, 130.05, 130.165625, 125.3, 126.490625, 130.590625, 131.89583333333334, 122.50625, 130.18854166666668, 127.70833333333333, 132.77708333333334, 130.74895833333332, 130.765625, 129.92708333333334, 126.525, 129.99895833333332, 127.06458333333333, 128.91770833333334, 129.815625, 126.62604166666667, 128.39895833333333, 129.31458333333333, 128.359375, 126.38125, 127.60833333333333, 129.62291666666667, 118.92916666666666, 126.98229166666667, 126.62708333333333, 127.55416666666666, 127.37083333333334, 125.51041666666667, 123.7, 125.88229166666666, 125.06666666666666, 124.82916666666667, 123.37916666666666, 126.071875, 123.38020833333333, 118.84479166666667, 120.828125, 123.20104166666667, 121.240625, 123.759375, 120.46458333333334, 124.56041666666667, 119.165625, 119.85104166666666, 125.18333333333334, 123.06875, 124.71875, 122.403125, 121.32916666666667, 121.76979166666666, 120.48125, 121.409375, 123.01041666666667, 121.77708333333334, 120.79270833333334, 119.51145833333334, 123.78333333333333, 119.53229166666667, 120.315625, 120.959375, 118.73125, 117.721875, 118.13125, 115.94479166666666, 118.82604166666667, 118.796875, 118.38541666666667, 119.428125, 114.84375, 115.39791666666666, 117.309375, 114.14479166666666, 115.48020833333334, 114.3125, 110.96145833333334, 110.79583333333333, 114.27916666666667, 108.76770833333333, 111.921875, 110.93958333333333, 112.30833333333334, 114.109375, 109.78958333333334, 113.27083333333333, 110.55104166666666])
    components = np.array([[0.12045456236398425, 0.13274743869959152, 0.12197307355675266, 0.12743299091720234, 0.11490209251739833, 0.14563737064685178, 0.13257213216265878, 0.10982553740327912, 0.1033285554752961, 0.13472662522721837, 0.13734539959726674, 0.10717686122229292, 0.0972455689737119, 0.09628519283479933, 0.14261904858919425, 0.1274903009982629, 0.10746528096890201, 0.10549243881559629, 0.14624097373324552, 0.08108330657246215, 0.11565117251515195, 0.08579984559652909, 0.07722077328005046, 0.07356429499109529, 0.07806792713561538, 0.1287830706405758, 0.0770938218445555, 0.12127314449485128, 0.10529145520977692, 0.0666737897880095, 0.12106663037789264, 0.10715468133676069, 0.06518153953203541, 0.06902913017385172, 0.12905097869304238, 0.09918042049938723, 0.0670503038556256, 0.12025846624581893, 0.08264605744884729, 0.1095198874982001, 0.10408321252730723, 0.08701059785262559, 0.1159165261853803, 0.12762053759983283, 0.11488463158159773, 0.10753555331889526, 0.06239375044176811, 0.034823058379854714, 0.02821482747719719, 0.03143676575752331, 0.14334186728122542, 0.1267335944406827, 0.03686502722948758, 0.13122688070663696, 0.11441436163465821, 0.1276718406937713, 0.08634401201400563, 0.13748343666463453, 0.128694146295196, 0.02373159202829569, 0.05472651466163828, 0.07333156527941177, 0.07007496928305562, 0.04208686952314014, 0.1214657842369948, 0.04442965017198557, 0.06312655648383185, 0.026230248378694764, 0.12260281057713285, 0.027896561794885897, 0.04593648103381148, 0.0946624014566409, 0.05982694826613285, 0.11309389353078224, 0.11660618736201625, 0.11133109128239174, 0.10309342278889524, 0.05930267586295026, 0.08657556461440363, 0.06582397440491987, 0.0738072514129298, 0.07233486958023887, 0.06803912018462172, 0.08114562436494083, 0.12379198451153776, 0.07393364108445967, 0.1240012483989432, 0.08259621292599414, 0.07575397097656476, 0.08680707790068476, 0.06958914690998033, 0.10565889068394682, 0.11843116996881459, 0.09056368294316583, 0.09582610184821727, 0.09883256528611857, 0.1020105774312687, 0.102298878258458, 0.10326276381580185, 0.10380850799693964], [0.07427695211631623, 0.039811882262788464, -0.03349637993629075, 0.07786228194018201, -0.04668111519257428, -0.04429491972822766, -0.06616004146776565, -0.05539772065500931, -0.07812219709386471, -0.0954502500976247, 0.028405697724875847, -0.09341173749111467, -0.08586524685221456, -0.03633013419234499, -0.06802162009429634, -0.11684891229065152, -0.11303816812619571, -0.10743968823164149, -0.04010572641021138, -0.08030950995566959, -0.1382645944719659, -0.10202808740785227, -0.09369053960007831, -0.08837503155059326, -0.08049639035345198, -0.1194818303048421, -0.07592769638889951, -0.148171739843941, -0.1423307349094092, -0.08718378633837494, -0.15062669160662345, -0.15776940925897492, -0.0947036959693789, -0.06382235583823519, -0.11334686969028257, -0.14174920744926087, -0.07009150723064844, 0.07103863887897335, -0.1341812731761232, -0.14863976326585523, -0.14601685383629293, -0.12879958053358803, -0.09275187591127693, -0.06790394078104754, -0.09629473166305665, -0.09744207087600869, -0.08898475593215555, -0.027503247631508732, -0.029412932682782222, -0.02156300367284392, 0.03779412889438717, -0.02729707920164131, -0.04812680782505255, -0.009169112735356214, 0.08173159150866774, -0.009226650762017504, 0.05318691366286438, 0.019987926246070327, 0.0026706068756526136, -0.01263329970725933, 0.04433032057334482, 0.054089944768277526, -0.06097541590118143, 0.02522927017425784, 0.1219033582910201, 0.029924094269158933, -0.038610018877666546, 0.0019078300387098898, 0.11921535197772462, 0.016775564869322034, 0.045428814635574924, 0.08856814323949107, 0.06321333193543278, 0.11648889565062699, 0.125526103394427, 0.0788354561418703, 0.055582546733798206, 0.08211279023904117, 0.058071048525764904, 0.08867322019446262, 0.0991915170010914, 0.0959334774800818, 0.09086193340213786, 0.07455462548062056, 0.11684583745026536, 0.12484032654355763, 0.12613985770587924, 0.1366128164934378, 0.10986248072218481, 0.12082727116476369, 0.1256353721159883, 0.15499526647986886, 0.13862112119684145, 0.17577415475772637, 0.18337757148419756, 0.19059198440380404, 0.19439996791019978, 0.1767974026432745, 0.18646453874321384, 0.1754916175670913], [-0.00933256141156874, 0.023120614961157915, 0.0693989313969695, 0.01320238045168485, 0.08287013529852502, -0.05719173269893544, -0.035953800288682725, 0.11788972007037492, 0.11117898797695132, -0.0452642918909738, -0.10116366389321987, 0.05225558452311782, 0.13427427317192686, 0.1397534015827757, -0.07242343692319783, -0.040964644302417814, -0.00015042935818411593, 0.022502483142039273, -0.09590072351874171, 0.18175424571136478, -0.04495378509408897, 0.07473305657100075, 0.14730941215950036, 0.1815032331989981, 0.18975890215123103, -0.07348865894980773, 0.1866256679593169, -0.06084908238234042, -5.814339640359373e-05, 0.181225679310337, -0.05398453319049602, -0.026784868749734826, 0.1581044699710331, 0.17672958032914846, -0.0806556076008471, -0.004352907835311327, 0.17996190012133742, 0.03218130192539418, 0.02605439661610908, -0.056070083325005536, -0.019477901682154665, -0.0234080180757932, -0.09251128540560188, -0.11393122047871658, -0.09599712630457427, -0.08840771275093223, 0.006796029985824794, 0.13927211803076323, 0.12192889818679055, 0.13499493138637106, -0.12036253989768227, -0.1472185783491534, 0.04533679746868221, -0.14667255699909962, 0.04290591743655356, -0.1498826878367511, 0.08724558056189287, -0.14489232304489794, -0.14859181490368206, 0.0846090308389735, 0.12793861507352094, 0.10387118208689004, -0.03334203735106001, 0.12860997603586816, 0.010892557854740343, 0.13585234272550817, -0.030742624780094256, 0.11436691275655281, 0.02903076356044326, 0.11724128429849787, 0.1370038557088258, 0.07454116849630293, 0.1289058938470337, 0.03382322702821473, 0.032933875329329766, -0.11902302581459101, -0.11381757701140115, 0.13643143061013904, -0.09255685848090382, 0.1354200149509334, 0.12618990609923333, 0.12605427466988936, 0.1311490116995681, -0.07224096602084207, -0.10699237669525903, 0.10526487698300684, -0.11863383448356166, 0.08185983076167806, -0.07325214189173063, -0.053945257756468655, 0.03839384328102334, -0.10927154338337101, -0.0964051943561984, 0.03893455101188017, 0.013733913498424264, 0.035494554136905804, 0.03668729307078847, -0.0216091032169731, 0.025559314929695934, 0.011426664880903218], [-0.23136933185503336, -0.2161020031042708, -0.18614653500540906, -0.21139272470533232, -0.18384493235520186, -0.1473728166876649, -0.11569871202261714, -0.15142756249926323, -0.12553444099310748, -0.08691636120402525, -0.16798106109277552, -0.06870366391362498, -0.11940729734616841, -0.10525042868656297, -0.06071405338000366, -0.04701581375877308, -0.031527354383220275, -0.045670329824784424, -0.08712493621008481, -0.0662479179585096, 0.03280338740468068, -0.005989226514360573, -0.04015225355074549, -0.052575068928006154, -0.06201255054896384, 0.009139574921949561, -0.05165349260558421, 0.04408083288719093, 0.053535220783568815, -0.024906349388148685, 0.060330599157271646, 0.06349041422671207, 0.0034333332000897037, -0.01838398396273624, 0.031190663566890986, 0.07740046596960165, -0.005771120586185805, -0.11166499412362511, 0.07174939430375468, 0.10481094707684382, 0.11459262854654928, 0.11577271935834052, 0.13387807033772428, 0.09377749863163286, 0.13974081005304714, 0.13943980254343646, 0.11069988427427516, 0.08848291733735344, 0.05623294235263613, 0.09605194955199742, -0.03774518705571989, 0.07929869449875707, 0.05469575015560545, 0.07975557398833436, -0.0525108514461137, 0.08310176948981614, 0.05968445492410431, 0.008213113802162948, 0.06953349899683414, 0.08027987569203661, 0.11477975798489026, 0.07104927585181656, 0.14458268752273934, 0.1546593657926004, -0.1302978643662246, 0.15105855466209336, 0.14424853244241057, 0.13786178490909318, -0.10199399099609359, 0.13319744494493838, 0.13941464120046043, 0.035344122047088554, 0.13851148548200126, -0.04362465701927064, -0.04018818503163499, 0.07099852579033225, 0.11080055792008016, 0.13769481525580754, 0.09918930692260736, 0.13882795644208157, 0.14111957849920137, 0.1432555357853417, 0.1364184612027381, 0.10025995635559283, 0.019386169175211455, 0.11289733853935753, -0.0018287142622586483, 0.07711110255685606, 0.08659532553714122, 0.06501048068275507, 0.0824184262300564, 0.030848667801378286, -0.04721815895296625, 0.028907488100513957, 0.005245586939032924, 0.01317394246796655, 0.008760230609220763, -0.0177006291572831, -0.04629341678740108, -0.0746177027149871], [0.037690048895080513, 0.06110901644853692, -0.02563939014606337, 0.07314304075321906, -0.040236179479766314, 0.09510885387480432, 0.08320258765694977, -0.05571163659022529, -0.06740748104299649, 0.11141473441359251, 0.05194649778048965, 0.007972035364389177, -0.09255412404517871, -0.0639157776646988, 0.11446200894873339, 0.10271520285444792, 0.06695870017450263, 0.043407442712117994, 0.07618864035713319, -0.1494222425830938, 0.1172316655543906, -0.062162179820383245, -0.13079400003184702, -0.16691833166610143, -0.1593614255345527, 0.12427005934826571, -0.15459458074548874, 0.10961225312185131, 0.09998789310933798, -0.1669161858251823, 0.09983000042887265, 0.10583006987965708, -0.14416557061183316, -0.12830334084395348, 0.09675121479434962, 0.07698603582322253, -0.14977048434429893, 0.052962353686343486, 0.045453669164962365, 0.05401825129613267, 0.05408480440673684, 0.01322945644898536, 0.018156612982707623, -2.643536993658978e-05, -0.0007624628175847783, -0.012365930856381538, -0.017403443008057764, -0.09510115775194311, -0.11054496707250897, -0.09933343603387611, -0.06288330478764137, -0.08178078983880593, -0.0879314907146351, -0.09396771286954639, 0.08045173550541573, -0.11698485253616438, 0.08753087008008031, -0.1105918422025972, -0.12791781476564604, -0.08615673813181468, 0.0215511179917506, 0.05504815118480838, -0.09194195191311737, -0.0067335044712435715, 0.08581343654393875, 0.0015137085479382806, -0.08937044235930146, -0.018613147916496296, 0.08345921639601772, -0.017350708994243912, 0.0386248617946886, 0.1268996168019844, 0.0907049133399694, 0.12772119088872336, 0.10953565114488233, -0.16477268576389428, -0.1551777779841097, 0.14359392779316307, -0.14850591230846186, 0.1662654007074684, 0.1623716089206027, 0.17105241625889933, 0.15165944441720045, -0.13679826278446872, -0.15319993381791844, 0.1478373159639389, -0.14656614748831373, 0.14269203060575847, -0.11787563275335376, -0.10539993789396958, -0.05541117326341737, -0.13165757654801322, -0.11849297781350455, -0.0005581150150285806, -0.05519449699625803, -0.01734966911117594, 0.0047845869500246965, -0.11093433471917709, 0.012868945440663509, 0.0045411143298072715], [-0.08544801279007816, -0.07451420811315274, -0.0016787920694400875, -0.10918954762612285, 0.0006645669368246037, 0.0130536051402424, 0.061630164752323996, -0.01938440747905539, 0.01762022192038726, 0.062242530120766776, -0.04818496696712314, 0.09149100144448237, 0.02774668262861, -0.037273894405343545, 0.008772515025383837, 0.08684843125972866, 0.09893429240832244, 0.1039922005322753, -0.025507714742932494, -0.033433668665663684, 0.10343841323704486, 0.09018278333101025, 0.03167979592272609, 0.012962333165191903, -0.025993574340877065, 0.05451130332856261, -0.024367717552821102, 0.0829361440860803, 0.1349676264621952, -0.0049073887867151665, 0.08909992707969659, 0.13554107672350457, 0.02827409405375425, -0.032198960504094494, 0.009861871182441225, 0.137168154760559, -0.009565414117235122, -0.15699598916553037, 0.11798607405510157, 0.057303050234343594, 0.10155142949331836, 0.04580661270982255, -0.07037105965985341, -0.10799611166033656, -0.0759387157210499, -0.06729899181428671, 0.04562748040318221, -0.004116047928152251, 0.006250635075361293, -0.014446986929008778, -0.09564315734295539, -0.16270692368649578, -0.00561550421082219, -0.18175519709100466, -0.1759027016780315, -0.1752126562788038, -0.13899294543621074, -0.14521024900411644, -0.1709331470776837, -0.006768417925238181, -0.14119063466329487, -0.14743558437406043, -0.021652307090078184, -0.11778345355759993, -0.1348511793029583, -0.10881316139267234, -0.007916248648151393, -0.04573341927655888, -0.15179543719313873, -0.03770827669746697, -0.07691380274892898, -0.13410460574783487, -0.04433648567361676, -0.13488999217757752, -0.10758877893204974, -0.04711261038890101, -0.04825861733951568, -0.016752918643401466, -0.017929802035230265, -0.02400215099187876, -0.02897542905775909, -0.019076724696111113, 0.008779835535542, 0.04676253592826912, 0.020362104140603462, 0.03462711470524901, 0.03220014242271659, 0.06442917200291763, 0.09423155820889059, 0.13414415572930322, 0.1517409186979024, 0.1140725348098644, 0.06651028414839588, 0.2216875721112911, 0.20708884779349615, 0.24060009169998578, 0.22623110932125032, 0.20687550375532346, 0.17441451786789547, 0.13369562916487354], [-0.1219760705647635, 0.0463132204007372, 0.1720429669526137, -0.052328156782684127, 0.19355837240780982, 0.046756002926844634, 0.09474011163159698, 0.15463964494704258, 0.18458943161721786, 0.06059138170460335, -0.07889111265496568, 0.10584048385658008, 0.14004140333517662, 0.07670508029491273, 0.013423286127894125, 0.03323544993929073, 0.06759346856905131, 0.06650399289526991, 0.021926623030574705, 0.02923369946529558, -0.04014394929487553, 0.015226065789636915, 0.003998888368380851, 0.019253289171357758, 0.027091874544370495, -0.038383794179748354, 0.005343334865500343, -0.06508058276124319, -0.07566863148725682, -0.028445901809636277, -0.08436696002671314, -0.08452514896700707, -0.05774176026323166, -0.02360127639836982, -0.05959899072351692, -0.08943128865118732, -0.05354731468146746, -0.09889359481261852, -0.07812257150019004, -0.08528545810122909, -0.0957264200692169, -0.06884811472369304, -0.026569119144643258, 0.020710364498611588, -0.019875690569265126, -0.031467986122785965, -0.060745946607770204, -0.14948340558967876, -0.18405259459324702, -0.16093204753072735, -0.011469733606887356, 0.05079606515754238, -0.10129946548159542, 0.04079505025653498, -0.09517781131572202, 0.046231132110970574, -0.08449054713667736, 0.038262832216934606, 0.04373688761898371, -0.170854495715837, -0.09447662509721509, -0.06528167358085407, 0.023092607858978285, -0.0943601465686493, -0.22817863461541463, -0.08641629431278632, 0.02562904328251365, -0.10456949095036924, -0.17754643117505894, -0.07264082802686594, -0.032979645352043584, 0.0158670834588933, 0.08176847518628272, -0.07503607825738418, -0.07727710652973158, 0.06630406925402084, 0.11384790914931686, 0.16526917457923326, 0.1276625441423596, 0.18840795939388472, 0.19996123653877743, 0.2050638144126683, 0.1828548383166045, 0.1437543367287414, 0.04460060228230856, 0.15464954792546315, 0.01705648060500097, 0.11689472459244661, 0.12153308057451014, 0.0879683164037787, 0.05718745046760029, -0.006545032907214883, -0.05289591263905744, -0.03680821135852304, -0.045767210351066796, -0.07575503795679364, -0.07948312371204176, -0.08787848827830701, -0.1872745934267767, -0.19370808342275783], [-0.09384509098756715, -0.18541606023633617, -0.2235587746836656, -0.07988232314845777, -0.189363743738677, -0.1604168002117627, -0.12258471226337789, -0.11755966767442672, -0.08314066676905627, -0.05490393869784472, -0.11684375790141348, -0.03502406561667093, -0.04483694415366581, 0.03594959272270451, -0.07199612550693085, 0.0028490552267156776, 0.010933760845479978, 0.020704248762714463, -0.10090057896546366, 0.13973838161935168, 0.06488112172502733, 0.0776743954532379, 0.08051823981240239, 0.13202518566657326, 0.150819866828187, -0.004126983004220285, 0.15221667755733412, 0.04924374329829007, 0.06743729928378406, 0.1143302096143318, 0.057340822676574636, 0.07716879736541245, 0.08865835495908402, 0.1377089494314405, 0.007281543897906287, 0.08334374799064963, 0.11606085146634346, 0.08959545145547657, 0.07633630514914845, 0.03486315247955768, 0.06042966995483555, 0.026249423947930506, -0.0500436598009625, -0.042326630122842296, -0.05415141872187776, -0.04917981620835973, -0.015858160648755965, -0.18339150330884357, -0.14714023903622256, -0.20472304747089332, -0.031044174601062634, -0.02691847925922923, -0.09869961220078524, -0.026273897258085606, 0.12283314000570823, -0.02054906099263297, 0.09750243283766148, -0.03234782026055294, -0.0034345696544453535, -0.12216577446256012, -0.08854984319622386, 0.0017750096101744606, -0.04749568450069632, -0.19715538008284214, 0.18315149651882828, -0.1988518482207045, -0.021348290946973308, -0.21520865610667272, 0.17316088831289644, -0.17554410944616156, -0.17124251577106997, 0.12748402299279396, -0.09353257299216866, 0.18814461390515874, 0.17828347913523213, 0.08181982192957603, 0.08669857996762943, -0.00861409921680064, 0.0776317494936701, 0.026041429133859373, 0.058057689459500204, 0.04440590833822983, 0.030885429127526388, 0.05687865655767271, 0.07854081430100102, 0.04385959707463271, 0.04485419360147986, 0.03645249466789311, 0.03440983967289243, 0.04057463801869179, 0.0013963291578930192, 0.0037115895556627003, -0.005286942664958748, -0.039384964196316534, -0.056413981525878315, -0.0506684763972911, -0.06348439823989749, -0.04730603878344295, -0.025024278382738917, 0.002136109915161956], [0.020819927808182502, 0.07242201144489469, 0.006014046648619906, 0.052692280013783156, -0.00657835857929763, 0.045865364761149746, 0.09248821725804464, -0.02354244796036903, -0.008919507458202365, 0.10222177615813656, 0.001884784353924233, 0.08608100968097358, -0.0524601889198646, -0.02322957639180695, -0.013670957874120343, 0.09888851799337434, 0.11629177503670167, 0.11805383704326267, -0.03508898517756673, -0.07199058404936304, 0.0893569564236647, 0.040895343200489584, -0.07188312998888932, -0.07950034949413268, -0.07392697799771607, 0.010901334562897597, -0.056130263612434964, 0.027666971109965514, 0.07993979279860239, -0.05099641999456283, 0.012184906336576713, 0.06102038938829712, -0.06329209483486976, -0.0007622225313691492, -0.061130105578385344, 0.06595324653274495, -0.04153216321798883, 0.0611883246945814, 0.05322352054265438, -0.02521564125707076, 0.021840476716621798, -0.058911301555617725, -0.16299515744845441, -0.16827079092468883, -0.1654989459671485, -0.15095085754521675, -0.011805710960467312, 0.004906811676346552, 0.0074838496244697085, 0.023433118297067696, -0.1018483131998501, -0.176108816961782, 0.062459023382112344, -0.17142580697117582, 0.09324836563719233, -0.1629844074641817, 0.13916896803281106, -0.12020517139098454, -0.14843956078464732, 0.04528846315269598, 0.15219282660482444, 0.1354185721464508, 0.005210837988896095, 0.1314112480632262, 0.02829512387596394, 0.14388090576207427, 0.014600091720581045, 0.07004528944958625, 0.029878284947081085, 0.08314833215221018, 0.10749869473450646, 0.07238264158025169, 0.02931456033562732, 0.016632542229326405, -0.006652310310716504, 0.1425740691138231, 0.14233337008922534, -0.02865464277919114, 0.1639552958002993, -0.056927224769012824, -0.06410660011980206, -0.0761534967643115, -0.07835409610660324, 0.193459584112172, 0.15963567370024567, -0.11451217267229602, 0.15541113441747745, -0.1054315525835646, 0.1840730764768977, 0.1922878620548792, 0.06671773326711576, 0.14794827475894282, 0.08156430216003108, -0.1709678746262989, -0.10002958441812522, -0.18405863959266056, -0.18963577556587627, -0.01847922450243764, -0.1719112198217033, -0.14191225325680226], [-0.11786729349836923, 0.008228239947118939, 0.17115589783096735, -0.009202487320133838, 0.19266641587525118, -0.14997692588119815, 0.0392762082353107, 0.16210262336969178, 0.12670287914305417, -0.0911325283784187, -0.2551004180754173, 0.14572695543144418, 0.11653397308830185, 0.031596694360077435, -0.16499506613419018, -0.06526095286594806, 0.10134443827431124, 0.12779951637161086, -0.19835522687722668, -0.11601580241425791, -0.06261797629896514, 0.11725835656023167, 0.009591250773861072, -0.050560171473570274, -0.13441899123307505, -0.15086997031009033, -0.14494489135136318, -0.10017371786573694, 0.013289116461401967, -0.09385011929859868, -0.08366489702999344, -0.028616932517739155, -0.032987681089740595, -0.12038648259414707, -0.13489074223122846, 0.044304159278805065, -0.12544030813272739, 0.06624089162617827, 0.08869467528634432, 0.025044770859774905, 0.0606300504096339, 0.1329837708078799, 0.10168558785053121, 0.021199796717683662, 0.11227711702146159, 0.12835687481234048, 0.17402725419964998, -0.10720197678114458, -0.10898190633773036, -0.1059931302326204, -0.12972740318381168, 0.04917361817340714, 0.08057976435257735, 0.023524794893666722, 0.19512825643287876, 0.022742651380948168, 0.1738118149269099, -0.05377837796751032, -0.0014268311059039069, -0.03116613717183192, 0.07674641430760033, 0.14716721963327922, 0.13251592747474492, -0.016104926750650582, 0.059009987282686194, 0.0026405358214120667, 0.11025828660874275, -0.06579112051980052, 0.09297834712439883, -0.06788424958504236, -0.011945637391204406, 0.08674677510538517, -0.06089572079817751, 0.08626488422640097, 0.06313860708774255, -0.03990335289296187, -0.01181710323215167, -0.11049295990206262, -0.019912863712885292, -0.1148873008018144, -0.1086566472735499, -0.09834340081003538, -0.09553050697983372, 0.00849675871732242, -0.07535905636354984, -0.06925437242175489, -0.08798567189434833, -0.038604420035512996, -0.01293466818606563, -0.031682139263038915, 0.038378304178693506, -0.04471635865487017, -0.0740172518599364, 0.12003258583085206, 0.07981467031841243, 0.10862310674878661, 0.11040746336562896, -0.012658765598436723, 0.01024027078716295, -0.01335087332828319], [-0.2513397146619068, -0.21422159875146976, -0.11983298293401749, -0.2177401585773792, -0.11220562147821606, 0.10329655494633593, 0.14249177812065766, -0.11662133814925993, 0.003786859081339651, 0.20900854918743592, -0.05016998949578472, 0.2010913943230525, 0.03293791865430868, -0.07628715222039732, 0.0717652011577087, 0.19213719678307378, 0.2187273137818434, 0.22088632300352468, 0.08380040835789401, -0.05775368563911677, 0.05385414696989584, 0.14432109433320647, 0.07154691101866407, 0.010229649539411564, -0.0500535350307136, -0.036760380022317886, -0.04644787430765146, -0.05894258138474855, -0.002682823593482545, 0.003811010079224792, -0.0916294488939777, -0.04540362883196436, 0.07332716025203523, -0.03394010786574071, -0.09339951064968072, -0.06069080417323323, 0.006020195170407457, -0.13842093619474333, -0.04189512875833535, -0.13889415491671694, -0.1141336435076624, -0.17115480339420008, -0.14097235766571117, -0.005386143871112417, -0.13295374037364263, -0.1451893332222384, -0.14988662942495667, 0.07391003598379577, 0.07100767246508805, 0.09623378484396368, 0.0854461613719982, 0.09777877534796466, 0.016217145719486792, 0.14052985568855927, 0.02175993841330336, 0.14791904379897425, 0.07602030578753008, 0.13418061101983783, 0.1673317134819667, 0.019786360315452427, 0.039221507288812386, 0.058328647242477105, -0.146413138170012, 0.020593924980513368, -0.013323002186391408, 0.011205906131295676, -0.13046223599526555, 0.04011465170969129, 0.04265402691955576, 0.0596838618626603, 0.03739321467999482, 0.08215257872459947, -0.007154707464619593, 0.10443557084752703, 0.11029952933493968, 0.043744581140079716, 0.017652072780890732, -0.0388471287035563, -0.017304810096413298, -0.05519359870824777, -0.05696191545581475, -0.05221264218894246, -0.05056383985137804, -0.07768813017840287, -0.045081980195663195, -0.014384054488029862, -0.046726620177391064, 0.002373733814482403, -0.11157388976674415, -0.07550498206821055, -0.023950193442225903, -0.07534282478859937, -0.07259511688853092, 0.09828697327198481, 0.06617607054946366, 0.08529306478270049, 0.07465543802906395, -0.023902700520894014, -0.00880430095988275, -0.03177260804421601], [-0.12255340864125157, 0.0021887822865964534, 0.031399376136131835, -0.08006191524942391, 0.02195326317346911, 0.0154720867254712, -0.00929907844018311, 0.03606282583033518, -0.02091948455332581, 0.04163359836042003, -0.17151248916251677, -0.09919337970118561, -0.0012748914224789921, 0.12644309233654055, 0.0801652771057474, 0.04978235397571914, -0.06630309305352264, -0.093747191221861, 0.015020586027325873, 0.12088685401078547, 0.07216346096783863, -0.18889987256725313, -0.06696573657693831, 0.01899422668747751, 0.12071421836781812, 0.11395738083389495, 0.12689789429850593, 0.1199175419028405, -0.011223336467285286, 0.03025224360379258, 0.11353586518322148, 0.01551622660606098, -0.10295590229647693, 0.10369236671730123, 0.1203009087710298, -0.07511830965386755, 0.04619307716320689, -0.057443995314841234, -0.13617591675709828, 0.03461705750304815, -0.049951006480153354, -0.022240518989906904, 0.08914776009152009, 0.0809430104245696, 0.07858419798347158, 0.07202278815291406, -0.22349510410023352, -0.15403246698118098, -0.19839318070309803, -0.14783699318518426, -0.09179661233026574, 0.023479611926100565, -0.2785099082982101, 0.002745271105208377, -0.0009024933052708843, -0.02408965362424361, 0.1505329600694793, -0.032182532648800666, -0.02656593079961144, -0.19412436226434168, 0.22419960665342523, 0.18425380989298926, -0.14477664601937823, 0.17453705234005445, -0.13993326398134223, 0.18675401420180562, -0.1621063441213844, -0.010307217821062264, -0.0938928213158728, 0.002777997116799885, 0.12884113873538622, 0.039065207205471054, 0.009692245726841818, -0.033274030406764875, -0.022399293490184647, -0.03714886913886047, -0.037115369322559544, -0.07237314872426523, -0.03789989189081548, -0.10439237172733204, -0.10531901603010527, -0.1107001663223393, -0.10932846580005016, -0.03560436744525201, 0.00010029344662279956, -0.12163555601062541, 0.009845171700438403, -0.08366427582713913, 0.027018959133811263, 0.019645351742238984, 0.05781284112465725, 0.0568039318038241, 0.013893129424120743, 0.10189884743514203, 0.11124441107788823, 0.10428252737521722, 0.0838270539553605, 0.09085650088533874, 0.0005851622818494205, -0.05342561151637326], [0.1298628024841948, 0.028941126771515502, 0.022643061955743095, 0.07157188232464941, 0.03288625148136739, -0.1731519059853523, -0.10476770613704306, 0.013230287989520055, 0.044946730602309554, -0.12829229246057997, -0.0019355594230001172, 0.029303994028417865, 0.026149212386844048, -0.033792132031216254, -0.20387165662779477, -0.07779072759387018, 0.03665504332034817, 0.0476884511848377, -0.15474368059892907, -0.05661330661216585, 0.08316168696755388, 0.0767476590061679, 0.06812605860481008, 0.03301714565950346, -0.05930576236133233, -0.09890588252149164, -0.058995396180482385, 0.019358139552143685, 0.1783442980791623, 0.04145819361499844, 0.03590677208524228, 0.1510163234841203, 0.02229916882705904, -0.0649908607604145, -0.09809903344397852, 0.1870709080126245, -0.03492775575143372, -0.009681459780901097, 0.15402714476555082, 0.02540649075703778, 0.11939730777436137, -0.008253707325313992, -0.15639697144548328, -0.06336068944396805, -0.1492937656238162, -0.13379584934624295, -0.005598317999027715, -0.14031637950450532, -0.1300919170955909, -0.12007298325971047, 0.20561041866855623, 0.04395884268212619, -0.07274520394470692, 0.10788743921086227, -0.043517637406183585, 0.1176866146285074, -0.05046971196769881, 0.20873930255739706, 0.13532440792248093, -0.13437490059326776, 0.059310350495176176, 0.025519594561994107, -0.14523831448886662, 0.11417414435745268, -0.03757680544555553, 0.11506170107893965, -0.14824651510658354, 0.018253111868849748, -0.05068070705901394, 0.003955824920981681, 0.1506492813405377, -0.08764075444560303, 0.11285626698852562, -0.13083623630807364, -0.11768765504364341, 0.00582764033307096, -0.051922097055611544, 0.10338618529041094, -0.1300242475970998, 0.08286061162745581, 0.05553786359847603, 0.06057966527291042, 0.04967195595513759, -0.17618605847248087, 0.09885656017379353, -0.06618276609651834, 0.14941147601871266, -0.11482841402618502, -0.1343903284813578, -0.12458418986015084, -0.1165426905320361, 0.03956216087607967, 0.17090324885719688, -0.05270315607408266, 0.006464584169981543, -0.01861128102656601, -0.018495484657761053, 0.11057190312426086, -0.0007022359419505998, 0.020636780395922833], [0.26876257828245365, 0.030342563994513433, -0.12651996147768133, 0.0038643656122664314, -0.15204057760112502, -0.0007641786114773947, 0.04240349428045237, -0.12398342714676804, -0.03160459489078769, 0.06960410309815325, 0.15183707446447356, 0.086276066558795, 0.009647115235141696, 0.04872956517253935, -0.015959122773132436, 0.08335328089461408, 0.14708872412473883, 0.13393239139252028, -0.031985401148430744, 0.02248724886192665, -0.008691290027579662, 0.06035420487588237, 0.05812338556665807, 0.05548948909449838, 0.018202065713419178, -0.0724052662981024, 0.010434544486390088, -0.09038917844485447, -0.01678886645904913, 0.06446967959190432, -0.10335964466427106, -0.03859977664152671, 0.07422664587390942, -0.03308732995540393, -0.07760356829722935, -0.023930686051869852, -0.0004465936845567908, -0.11597531937577653, 0.0105660658921454, -0.05753403788205574, -0.056238909224064444, -0.04835346745876173, 0.03219839544587909, 0.019865591445006478, 0.06677730419693688, 0.09873453565827434, -0.040148072882106374, -0.14690430305458907, -0.1907459542134044, -0.14567273517758497, -0.07295984375072026, 0.014146438032651905, -0.14522710335782665, -0.019634535250515443, -0.1429229825070578, -0.023029066155944126, -0.028649697269900007, -0.09700280778141895, -0.028243530918768203, -0.14171546955384295, 0.06732394563060075, 0.04066035628824726, 0.16280343329888372, 0.13488643563326172, 0.0747280396858497, 0.1281390261172496, 0.20428891230035912, 0.02905812708784834, -0.06258389265271744, 0.031060826384523704, 0.08940228129998092, -0.05138321075623721, 0.03252202845372888, -0.08206193283858289, -0.09214388341189496, 0.09790616147253645, 0.16264193035890315, 0.007395834179908272, 0.15511228441896663, -0.00875267901030373, -0.017143275294465105, -0.019464706406866553, -0.0324138671009076, 0.07860514635157385, -0.14109596817707576, -0.00425161006613583, -0.14978026817413273, -0.03042722727156392, 0.01844418833659248, -0.03659696309045114, -0.08939745824952111, -0.12266909764072759, -0.1250560355230883, -0.08426212362554744, -0.1134794858513295, -0.05910804294480634, 0.02576699867172425, -0.02054271780699461, 0.3320744287551894, 0.3127870849623764], [-0.17288895539991406, -0.0756078147157021, 0.049396044023202385, -0.2336331855175755, 0.07067810266157508, 0.08170835762951008, 0.06076422306365617, 0.13316809616343675, -0.01400107701185431, 0.05275563192995514, 0.031049386146804966, -0.025351670290182142, -0.029933862363672525, 0.11676754626075742, 0.06414258271616012, 0.039904997131366575, -0.03719037923015189, -0.06756570967387232, 0.05732909421897486, 0.02949301559664947, -0.02895609692913142, -0.07573251054778088, -0.07494554243742606, -0.05032558108570399, 0.026358184145553507, -0.03467871066082566, 0.02061355059402574, -0.03858996521952543, -0.056955765675628524, -0.07138882150690483, -0.036934790186343935, -0.03606759245068456, -0.074295087976826, 0.03283550204483976, -0.03683364067682982, -0.013391083712326085, 0.011617107593594389, -0.2174452780977087, 0.03192953152593199, 0.0007725136950456338, -0.0066609031325660295, 0.08572885781438543, 0.007859723467264127, -0.031090400259929825, -0.0012035664390872224, 0.01709524698635168, 0.13710780051128046, 0.06571625918470327, 0.019260190038357886, 0.046485917090559374, 0.014819113416082925, -0.09263158817067929, 0.09525995358912832, -0.10658622247805265, -0.05622522441827199, -0.08649878936026194, 0.11242049973028695, -0.034413170345594456, -0.06180537672875694, 0.009894052898008597, 0.06518325892847163, 0.1599519022309648, 0.10588735109310851, -0.03662409389826212, -0.0854301778219907, -0.041271426125800904, 0.08823460986365293, -0.15701463683137087, -0.07944935388015614, -0.22414271025206442, -0.008819547257363674, 0.09769084189707455, 0.09685380346255676, 0.06144921258465265, 0.05410545716601243, 0.10481665219720551, 0.06124774036686552, 0.08352299619872237, 0.004210414821041102, 0.05116340146676279, 0.04809561884010102, 0.01698274702901268, -0.03258881656896566, -0.12585495731341317, 0.15587666421076607, -0.061736358102379374, 0.248693109919927, -0.02263524434889114, -0.2686114860347778, -0.2450184069334655, -0.21803343561338492, 0.06824304478948476, 0.2650407594332913, -0.08155938776391551, -0.14875191966478066, -0.04775711720504308, -0.004541582065360795, -0.00044579678299426294, 0.1663201750320026, 0.21787280547976334], [-0.08754798619481767, -0.12832845244408345, 0.13323612703474833, -0.21448517192647615, 0.15265009215178987, -0.12129320411260618, -0.16631881863944734, 0.09301172122917914, 0.2210791218270511, -0.07767094597606858, 0.011532335592580092, 0.05586513396234395, 0.2192133767782956, -0.08559390671662943, 0.014414094589677594, -0.08928183678446433, -0.14095005810540612, -0.14801547683713567, -0.04719896611352715, -0.11604322409872238, 0.060814991995059735, 0.046420776379022914, 0.15728257647082539, 0.07493937137544457, -0.11097025840787006, 0.191526493512392, -0.16745280593721545, 0.20940498038798916, -0.048901981652054435, 0.010079784029261355, 0.20237083518323842, 0.01799518157892194, 0.10196230562217608, -0.1618123749483487, 0.1894301468365986, -0.11033488039794051, -0.10875823406243793, -0.0663085651149928, -0.14572967902962491, 0.07055751537037827, -0.08108382694405651, -0.055844294541792704, -0.039062039772478886, -0.011731108784229767, -0.0450396996279873, -0.04673631360928826, -0.07590418169681583, 0.002592348211242983, 0.0031369060145144357, 0.014567955056615841, 0.016274320283579956, -0.058602526633528065, 0.029063948902160273, -0.04106660642290311, -0.01910543498035067, -0.027806456312734415, -0.031530615069528777, 0.027571647749343908, -0.005631303792164935, 0.03921003483827847, -0.060704423742544804, -0.07611266514271857, -0.09215584828551776, 0.05374475122012834, 0.08546068080980221, 0.0643672937382262, -0.07958415395662226, 0.025847737684194846, 0.08709978821859424, 0.017306617576897474, 0.04309971851412797, 0.005086329204172679, 0.04014090266604135, 0.11443775041532882, 0.1555833387818137, 0.049864511502373365, 0.09161686217807591, 0.03754513990863689, 0.08404447358643156, 0.015317154016354948, -0.0020449135400089145, 0.0005530452417903735, -0.023890490601852877, 0.07625884478104906, -0.01099826846083527, -0.07771317485990883, -0.03330205765715765, -0.06343648543240135, 0.09424603980247878, 0.06410183085805424, -0.03693814491816669, -0.07347764246007533, -0.14942680153998456, -0.11369026551307822, -0.104715101461197, -0.11665765958217217, -0.06697386380140852, 0.08841125237088782, 0.18212810571893265, 0.21926960834844658], [0.051512408798651675, 0.03849516913938012, -0.10940304299778858, 0.06322203748030465, -0.10417396115656843, 0.03474730041039886, -0.16882150173946384, -0.07642937022031461, 0.06916120446645031, -0.05612366202117828, 0.09661014610006398, -0.010397808557138847, 0.1158670481153929, -0.07221658001275799, 0.19257256851771773, -0.08669553400906473, -0.09922027687090959, -0.0883446868276571, 0.22478489071856855, -0.018489334404928903, 0.021395705989752756, 0.14429453407849274, 0.19522497771609704, 0.12250219547649194, -0.04753354770556419, 0.08651016897029144, -0.08914591875630748, 0.043911170183270745, -0.04686633390714434, 0.08274784179043665, 0.040467282564797445, -0.005280480443943599, 0.1655413456167464, -0.14552784360376808, 0.0756386642991008, -0.08171254048881656, -0.08223351201634574, -0.0032001449159120244, -0.06331317093438146, -0.02642462744228548, -0.048152098203007865, -0.00846896221406995, -0.0907927591241589, -0.13170229263396346, -0.0838003543348731, -0.05601389346572892, 0.06213542022011505, -0.13667859945939365, -0.12232590135257776, -0.13741975848989693, 0.01805536141395218, -0.06717233836765377, 0.018487632734106346, -0.07334825445610847, -0.05463168899374344, -0.06264894716010859, 0.039591140313472405, 0.00110890050456352, -0.04883412149731063, 0.012764493894801941, 0.04471382382036998, 0.021273059136432196, 0.2817542064876368, 0.08301399970083555, -0.06714477409202893, 0.10652087601493548, 0.28273348311443675, -0.011194564228291893, -0.0179186343529452, -0.014776284647065327, 0.023691874342185757, 0.053295697590956065, -0.008147313875618428, 0.04374257804609407, 0.03888383059362039, 0.023185469430060893, 0.021320919018949766, -0.035553825311698165, -0.0257254602055571, -0.04208078730936514, -0.033567856689518887, -0.03260048629469819, -0.01663140844438769, -0.055708429138893685, 0.038497193855231725, 0.021810955852252848, 0.10629152938838961, 0.013782204729992423, -0.11853335696692067, -0.14495869231829206, -0.032394895548047774, -0.03567413075496643, 0.10139717582711151, 0.22340155161948036, 0.06937140469363921, 0.1730405476655135, 0.10932660882299829, -0.12952887583278286, -0.2384563639648442, -0.23948946168932592]])
    explained_variance = np.array([109449.66861784918, 32388.609445327267, 24169.93681605564, 19762.47597835777, 15354.174924137156, 13804.822859664231, 9910.576736581419, 8432.20104427652, 8069.266010697484, 6801.484550989703, 6476.203094181213, 5639.610046725533, 5358.230858730513, 4553.46422136783, 4188.35562097799, 4110.559796410708, 3750.0683865862247])
    X = X - mean
    X_transformed = np.dot(X, components.T)
    return X_transformed


def __convert(cell):
    value = str(cell)
    if value == random_filler_value:
        value = ''
    try:
        result = int(value)
        return result
    except ValueError:
        try:
            result = float(value)
            if math.isnan(result):
                raise PredictorError('NaN value found. Aborting.', code=1)
            return result
        except ValueError:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            return result
        except Exception as e:
            raise e


def __get_key(val, dictionary):
    if dictionary == {}:
        return val
    for key, value in dictionary.items():
        if val == value:
            return key
    if val not in dictionary.values():
        raise PredictorError(f"Label {val} key does not exist", code=2)


def __confusion_matrix(y_true, y_pred, json):
    stats = {}
    labels = np.array(list(mapping.keys()))
    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    for class_i in range(n_classes):
        class_i_label = __get_key(class_i, mapping)
        stats[int(class_i)] = {}
        class_i_indices = np.argwhere(y_true == class_i_label)
        not_class_i_indices = np.argwhere(y_true != class_i_label)
        # None represents N/A in this case
        stats[int(class_i)]['TP'] = TP = int(np.sum(y_pred[class_i_indices] == class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['FN'] = FN = int(np.sum(y_pred[class_i_indices] != class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['TN'] = TN = int(np.sum(y_pred[not_class_i_indices] != class_i_label)) if not_class_i_indices.size > 0 else None
        stats[int(class_i)]['FP'] = FP = int(np.sum(y_pred[not_class_i_indices] == class_i_label)) if not_class_i_indices.size > 0 else None
        if TP is None or FN is None or (TP + FN == 0):
            stats[int(class_i)]['TPR'] = None
        else:
            stats[int(class_i)]['TPR'] = (TP / (TP + FN))
        if TN is None or FP is None or (TN + FP == 0):
            stats[int(class_i)]['TNR'] = None
        else:
            stats[int(class_i)]['TNR'] = (TN / (TN + FP))
        if TP is None or FP is None or (TP + FP == 0):
            stats[int(class_i)]['PPV'] = None
        else:
            stats[int(class_i)]['PPV'] = (TP / (TP + FP))
        if TN is None or FN is None or (TN + FN == 0):
            stats[int(class_i)]['NPV'] = None
        else:
            stats[int(class_i)]['NPV'] = (TN / (TN + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['F1'] = None
        else:
            stats[int(class_i)]['F1'] = ((2 * TP) / (2 * TP + FP + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['TS'] = None
        else:
            stats[int(class_i)]['TS'] = (TP / (TP + FP + FN))

    if not report_cmat:
        return np.array([]), stats

    label_to_ind = {label: i for i, label in enumerate(labels)}
    y_pred = np.array([label_to_ind.get(x, n_classes + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_classes + 1) for x in y_true])

    ind = np.logical_and(y_pred < n_classes, y_true < n_classes)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    sample_weight = sample_weight[ind]

    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_classes, n_classes), dtype=np.int64).toarray()
    with np.errstate(all='ignore'):
        cm = np.nan_to_num(cm)

    return cm, stats


def __preprocess_and_clean_in_memory(arr):
    clean_arr = np.zeros((len(arr), len(important_idxs)))
    for i, row in enumerate(arr):
        try:
            row_used_cols_only = [row[i] for i in important_idxs]
        except IndexError:
            error_str = f"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {len(ignorecolumns) + len(important_idxs)})."
            if len(arr) == num_attr and len(arr[0]) != num_attr:
                error_str += "\n\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' "
                error_str += "rather than as an element of a list. Make sure that even single instances "
                error_str += "are enclosed in a list. Example: predict_in_memory(0) is invalid but "
                error_str += "predict_in_memory([0]) is valid."
            raise PredictorError(error_str, 3)
        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]
    return clean_arr


def __classify(arr, return_probabilities=False):
    print(arr.shape)
    print(w_h.T.shape)
    h = np.dot(arr, w_h.T) + b_h
    relu = np.maximum(h, np.zeros_like(h))
    out = np.dot(relu, w_o.T) + b_o
    if return_probabilities:
        exp_o = np.zeros((out.shape[0],))
        idxs_negative = np.argwhere(out < 0.).reshape(-1)
        if idxs_negative.shape[0] > 0:
            exp_o[idxs_negative] = 1. - np.exp(-np.fmax(out[idxs_negative], 0)).reshape(-1) / (1. + np.exp(-np.abs(out[idxs_negative]))).reshape(-1)
        idxs_positive = np.argwhere(out >= 0.).reshape(-1)
        if idxs_positive.shape[0] > 0:
            exp_o[idxs_positive] = np.exp(np.fmin(out[idxs_positive], 0)).reshape(-1) / (1. + np.exp(-np.abs(out[idxs_positive]))).reshape(-1)
        exp_o = exp_o.reshape(-1, 1)
        output = np.concatenate((1. - exp_o, exp_o), axis=1)
    else:
        output = (out >= 0).astype('int').reshape(-1)
    return output



def __validate_kwargs(kwargs):
    for key in kwargs:

        if key not in ['return_probabilities']:
            raise PredictorError(f'{key} is not a keyword argument for Brainome\'s {classifier_type} predictor. Please see the documentation.', 4)


def __validate_data(row_or_arr, validate, row_num=None):
    if validate:
        expected_columns = expected_feature_cols + 1
    else:
        expected_columns = expected_feature_cols

    input_is_array = isinstance(row_or_arr, np.ndarray)
    n_cols = row_or_arr.shape[1] if input_is_array else len(row_or_arr)

    if n_cols != expected_columns:

        if row_num is None:
            err_str = f"Your data contains {n_cols} columns but {expected_columns} are required."
        else:
            err_str = f"At row {row_num}, your data contains {n_cols} columns but {expected_columns} are required."

        if validate:
            err_str += " The predictor's validate() method works on data that has the same columns in the same order as were present in the training CSV."
            err_str += " This includes the target column and features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {expected_feature_cols - len(important_idxs)} unused features are present in the data."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {expected_feature_cols - len(important_idxs)} unused features are present in the data as well as the target column. "
            elif n_cols == len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column present in the data. "
            err_str += " To make predictions, see the predictor's predict() method."
        else:
            err_str += " The predictor's predict() method works on data that has the same feature columns in the same relative order as were present in the training CSV."
            err_str += " This DOES NOT include the target column but DOES include features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {expected_feature_cols - len(important_idxs)} unused features are present in the data and that the target column is not present."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {expected_feature_cols - len(important_idxs)} unused features are present in the data."
            elif n_cols == 1 + len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column is not present."
            err_str += " To receive a performance summary, instead of make predictions, see the predictor's validate() method."

        raise PredictorError(err_str, 5)

    else:

        if not input_is_array:
            return row_or_arr


def __write_predictions(arr, header, headerless, trim, outfile=None):
    predictions = predict(arr)
    buff = []

    if not headerless:
        if trim:
            header = ','.join([header[i] for i in important_idxs] + ['Prediction'])
        else:
            header = ','.join(header.tolist() + ['Prediction'])
        if outfile is None:
            print(header)
        else:
            print(header, file=outfile)

    for row, prediction in zip(arr, predictions):
        if trim:
            row = [f'"{row[i]}",' if ',' in row[i] else f'{row[i]},' for i in important_idxs]
        else:
            row = [f'"{field}",' if ',' in field else f'{field},' for field in row]
        row.append(prediction)
        buff.extend(row)
        if len(buff) >= IOBUFF:
            if outfile is None:
                print(''.join(buff))
            else:
                print(''.join(buff), file=outfile)
            buff = []
        else:
            buff.append('\n')
    if len(buff) > 0:
        if outfile is None:
            print(''.join(buff))
        else:
            print(''.join(buff), file=outfile)


def load_data(csvfile, headerless, validate):
    """
    Parameters
    ----------
    csvfile : str
        The path to the CSV file containing the data.

    headerless : bool
        True if the CSV does not contain a header.

    validate : bool
        True if the data should be loaded to be used by the predictor's validate() method.
        False if the data should be loaded to be used by the predictor's predict() method.

    Returns
    -------
    arr : np.ndarray
        The data (observations and labels) found in the CSV without any header.

    data : np.ndarray or NoneType
        None if validate is False, otherwise the observations (data without the labels) found in the CSV.

    labels : np.ndarray or NoneType
        None if the validate is False, otherwise the labels found in the CSV.

    header : np.ndarray or NoneType
        None if the CSV is headerless, otherwise the header.
    """

    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        arr = np.array([__validate_data(row, validate, row_num=i) for i, row in enumerate(csv.reader(csvinput)) if row != []], dtype=str)
    if headerless:
        header = None
    else:
        header = arr[0]
        arr = arr[1:]
    if validate:
        labels = np.char.strip(arr[:, target_column], chars=" \"\'")
        feature_columns = [i for i in range(arr.shape[1]) if i != target_column]
        data = arr[:, feature_columns]
    else:
        data, labels = None, None

    if validate and ignorelabels != []:
        idxs_to_keep = np.argwhere(np.logical_not(np.isin(labels, ignorelabels))).reshape(-1)
        labels = labels[idxs_to_keep]
        data = data[idxs_to_keep]

    return arr, data, labels, header


def predict(arr, remap=True, **kwargs):
    """
    Parameters
    ----------
    arr : list[list]
        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'. This
        should contain all the features that were present in the training data,
        regardless of whether or not they are used by the model, with the same
        relative order as in the training data. There should be no target column.


    remap : bool
        If True and 'return_probs' is False, remaps the output to the original class
        label. If 'return_probs' is True this instead adds a header indicating which
        original class label each column of output corresponds to.

    **kwargs :
        return_probabilities : bool
            If true, return class membership probabilities instead of classifications.

    Returns
    -------
    output : np.ndarray

        A numpy array of
            1. Class predictions if 'return_probabilities' is False.
            2. Class probabilities if 'return_probabilities' is True.

    """
    if not isinstance(arr, np.ndarray) and not isinstance(arr, list):
        raise PredictorError(f'Data must be provided to \'predict\' and \'validate\' as a list or np.ndarray, but an input of type {type(arr).__name__} was found.', 6)
    if isinstance(arr, list):
        arr = np.array(arr, dtype=str)
    print('here', arr.shape)

    kwargs = kwargs or {}
    __validate_kwargs(kwargs)
    __validate_data(arr, False)
    remove_bad_chars = lambda x: str(x).replace('"', '').replace(',', '').replace('(', '').replace(')', '').replace("'", '')
    arr = [[remove_bad_chars(field) for field in row] for row in arr]

    arr = __preprocess_and_clean_in_memory(arr)
    print(arr.shape)
    arr = __transform(arr)
    print(arr.shape)
    output = __classify(arr, **kwargs)

    if remap:
        if kwargs.get('return_probabilities'):
            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)
            output = np.concatenate((header, output), axis=0)
        else:
            output = np.array([__get_key(prediction, mapping) for prediction in output])

    return output


def validate(arr, labels):
    """
    Parameters
    ----------
    cleanarr : np.ndarray
        An array of float values that has undergone each pre-
        prediction step.

    Returns
    -------
    count : int
        A count of the number of instances in cleanarr.

    correct_count : int
        A count of the number of correctly classified instances in
        cleanarr.

    numeachclass : dict
        A dictionary mapping each class to its number of instances.

    outputs : np.ndarray
        The output of the predictor's '__classify' method on cleanarr.
    """
    print(arr.shape)
    predictions = predict(arr)
    correct_count = int(np.sum(predictions.reshape(-1) == labels.reshape(-1)))
    count = predictions.shape[0]
    
    class_0, class_1 = __get_key(0, mapping), __get_key(1, mapping)
    num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0
    num_TP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_1)))
    num_TN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_0)))
    num_FN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_1)))
    num_FP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_0)))
    num_class_0 = int(np.sum(labels.reshape(-1) == class_0))
    num_class_1 = int(np.sum(labels.reshape(-1) == class_1))
    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, predictions


def __main():
    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    arr, data, labels, header = load_data(csvfile=args.csvfile, headerless=args.headerless, validate=args.validate)

    if not args.validate:
        __write_predictions(arr, header, args.headerless, args.trim)
    else:

        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(data, labels)

        classcounts = np.bincount(np.array([mapping[label.strip()] for label in labels], dtype='int32')).reshape(-1)
        class_balance = (classcounts[np.argwhere(classcounts > 0)] / arr.shape[0]).reshape(-1).tolist()
        best_guess = round(100.0 * np.max(class_balance), 2)
        H = float(-1.0 * sum([class_balance[i] * math.log(class_balance[i]) / math.log(2) for i in range(len(class_balance))]))
        modelacc = int(float(correct_count * 10000) / count) / 100.0
        mtrx, stats = __confusion_matrix(np.array(labels).reshape(-1), np.array(preds).reshape(-1), args.json)

        if args.json:
            json_dict = {'instance_count': count,
                         'classifier_type': classifier_type,
                         'classes': n_classes,
                         'number_correct': correct_count,
                         'accuracy': {
                             'best_guess': (best_guess/100),
                             'improvement': (modelacc - best_guess)/100,
                              'model_accuracy': (modelacc/100),
                         },
                         'model_capacity': model_cap,
                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,
                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,
                         'shannon_entropy_of_labels': H,
                         'class_balance': class_balance,
                         'confusion_matrix': mtrx.tolist(),
                         'multiclass_stats': stats}

            print(json.dumps(json_dict))
        else:
            pad = lambda s, length, pad_right: str(s) + ' ' * max(0, length - len(str(s))) if pad_right else ' ' * max(0, length - len(str(s))) + str(s)
            labels = np.array(list(mapping.keys())).reshape(-1, 1)
            max_class_name_len = max([len(clss) for clss in mapping.keys()] + [7])

            max_TP_len = max([len(str(stats[key]['TP'])) for key in stats.keys()] + [2])
            max_FP_len = max([len(str(stats[key]['FP'])) for key in stats.keys()] + [2])
            max_TN_len = max([len(str(stats[key]['TN'])) for key in stats.keys()] + [2])
            max_FN_len = max([len(str(stats[key]['FN'])) for key in stats.keys()] + [2])

            cmat_template_1 = "    {} | {}"
            cmat_template_2 = "    {} | " + " {} " * n_classes
            acc_by_class_template_1 = "    {} | {}  {}  {}  {}  {}  {}  {}  {}  {}  {}"

            acc_by_class_lengths = [max_class_name_len, max_TP_len, max_FP_len, max_TN_len, max_FN_len, 7, 7, 7, 7, 7, 7]
            acc_by_class_header_fields = ['target', 'TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'PPV', 'NPV', 'F1', 'TS']
            print("Classifier Type:                    Neural Network")

            print(f"System Type:                        {n_classes}-way classifier\n")

            print("Accuracy:")
            print("    Best-guess accuracy:            {:.2f}%".format(best_guess))
            print("    Model accuracy:                 {:.2f}%".format(modelacc) + " (" + str(int(correct_count)) + "/" + str(count) + " correct)")
            print("    Improvement over best guess:    {:.2f}%".format(modelacc - best_guess) + " (of possible " + str(round(100 - best_guess, 2)) + "%)\n")

            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + " bits/bit")

            if report_cmat:
                max_cmat_entry_len = len(str(int(np.max(mtrx))))
                mtrx = np.concatenate((labels, mtrx.astype('str')), axis=1).astype('str')
                max_pred_len = (mtrx.shape[1] - 1) * max_cmat_entry_len + n_classes * 2 - 1
                print("\nConfusion Matrix:\n")
                print(cmat_template_1.format(pad("Actual", max_class_name_len, False), "Predicted"))
                print(cmat_template_1.format("-" * max_class_name_len, "-" * max(max_pred_len, 9)))
                for row in mtrx:
                    print(cmat_template_2.format(
                        *[pad(field, max_class_name_len if i == 0 else max_cmat_entry_len, False) for i, field in enumerate(row)]))

            print("\nAccuracy by Class:\n")
            print(acc_by_class_template_1.format(
                *[pad(header_field, length, False) for i, (header_field, length) in enumerate(zip(acc_by_class_header_fields, acc_by_class_lengths))]))
            print(acc_by_class_template_1.format(
                *["-" * length for length in acc_by_class_lengths]))

            pct_format_string = "{:8.2%}"      # width = 8, decimals = 2
            for raw_class in mapping.keys():
                class_stats = stats[int(mapping[raw_class.strip()])]
                TP, FP, TN, FN = class_stats.get('TP', None), class_stats.get('FP', None), class_stats.get('TN', None), class_stats.get('FN', None)
                TPR = pct_format_string.format(class_stats['TPR']) if class_stats['TPR'] is not None else 'N/A'
                TNR = pct_format_string.format(class_stats['TNR']) if class_stats['TNR'] is not None else 'N/A'
                PPV = pct_format_string.format(class_stats['PPV']) if class_stats['PPV'] is not None else 'N/A'
                NPV = pct_format_string.format(class_stats['NPV']) if class_stats['NPV'] is not None else 'N/A'
                F1 = pct_format_string.format(class_stats['F1']) if class_stats['F1'] is not None else 'N/A'
                TS = pct_format_string.format(class_stats['TS']) if class_stats['TS'] is not None else 'N/A'
                line_fields = [raw_class, TP, FP, TN, FN, TPR, TNR, PPV, NPV, F1, TS]
                print(acc_by_class_template_1.format(
                    *[pad(field, length, False) for i, (field, length) in enumerate(zip(line_fields, acc_by_class_lengths))]))


if __name__ == "__main__":
    try:
        __main()
    except PredictorError as e:
        print(e, file=sys.stderr)
        sys.exit(e.code)
    except Exception as e:
        print(f"An unknown exception of type {type(e).__name__} occurred.", file=sys.stderr)
        sys.exit(-1)
